{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b512fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "\n",
    "def extract_full_dates(text):\n",
    "    \"\"\"\n",
    "    提取 sent 中所有完整日期短语（返回原文短语列表，法语格式）\n",
    "    \"\"\"\n",
    "    # 匹配 \"1er février 1877\"、\"2 mars 1912\"、\"01 avril 1855\" 等\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    results = []\n",
    "    for m in re.finditer(pattern, text, re.IGNORECASE):\n",
    "        phrase = text[m.start():m.end()]\n",
    "        # 只保留能被 dateparser 正确解析为日期的\n",
    "        dt = dateparser.parse(phrase, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            results.append(phrase.strip())\n",
    "    return results\n",
    "\n",
    "def replace_incomplete_dates(sent, triples):\n",
    "    # 提取句中所有完整日期短语（如“1er février 1877”）\n",
    "    full_dates = extract_full_dates(unidecode(sent))\n",
    "    # 构建映射表 { (年,月): 原始短语 }\n",
    "    mapping = {}\n",
    "    for d in full_dates:\n",
    "        dt = dateparser.parse(d, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            key = (str(dt.year), str(dt.month).zfill(2))\n",
    "            mapping[key] = d.strip()\n",
    "\n",
    "    # 批量处理每个 triple\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", obj)\n",
    "        if m:\n",
    "            year, month = m.group(1), m.group(2)\n",
    "            if (year, month) in mapping:\n",
    "                # 替换成句中真实时间短语\n",
    "                triple[\"obj\"] = mapping[(year, month)]\n",
    "        # 如需处理 yyyy-mm-dd，可继续补充\n",
    "        m2 = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", obj)\n",
    "        if m2:\n",
    "            year, month, day = m2.group(1), m2.group(2), int(m2.group(3))\n",
    "            for dstr, dphrase in mapping.items():\n",
    "                if dstr == (year, month):\n",
    "                    if str(day) in dphrase or f\"{day:02d}\" in dphrase or (day == 1 and \"1er\" in dphrase):\n",
    "                        triple[\"obj\"] = dphrase\n",
    "    return triples\n",
    "\n",
    "# ====== 文件批量处理 ======\n",
    "\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"    # 输入文件路径\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"  # 输出文件路径\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = replace_incomplete_dates(d[\"sent\"], d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"处理完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eb25679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全部处理完毕！\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "def fr_date_to_iso(date_str):\n",
    "    dt = dateparser.parse(date_str, languages=['fr'])\n",
    "    if dt:\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return date_str\n",
    "\n",
    "def update_triples_date_obj(triples):\n",
    "    # 对所有 triple 的 obj，若是完整法语日期短语，替换为ISO\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        # 判断是不是法语日期（如“1er fevrier 1877”或“21 mai 1956”等）\n",
    "        if re.fullmatch(pattern, obj, re.IGNORECASE):\n",
    "            iso_date = fr_date_to_iso(obj)\n",
    "            triple[\"obj\"] = iso_date\n",
    "    return triples\n",
    "\n",
    "# ========== 批量处理JSONL文件 ==========\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = update_triples_date_obj(d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"全部处理完毕！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "606801eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tous les filtres ont été appliqués avec succès. Le fichier de sortie a été enregistré à l’emplacement suivant ： C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "# prétraitement du jeu de donées pour supprimer des triplets raisonnés\n",
    "#ex. à supprimer: \"sub\": \"Boulevard de Sébastopol\", \"rel\": \"isLandmarkType\", \"obj\": \"thoroughfare\" \n",
    "#\"sub\": \"Boulevard de Sébastopol\", \"rel\": \"hasGeometryChangeOn\", \"obj\": \"noTime\"\n",
    "# si le sujet et l'objet entre la relation \"hasNewName\" sont pareils, on supprime le triplet. {\"sub\": \"Pont Alexandre III\", \"rel\": \"hasNewName\", \"obj\": \"pont Alexandre III\"}\n",
    "import json\n",
    "\n",
    "# changer le chemin d'accès aux fichiers d'entrée et de sortie\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_train.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        if \"triples\" in data:\n",
    "            new_triples = []\n",
    "            for triple in data[\"triples\"]:\n",
    "                rel = triple.get(\"rel\", \"\")\n",
    "                obj = triple.get(\"obj\", \"\")\n",
    "                sub = triple.get(\"sub\", \"\")\n",
    "\n",
    "                # 1：sauter isLandmarkType\n",
    "                if rel == \"isLandmarkType\":\n",
    "                    continue\n",
    "                # 2：sauter obj == noTime\n",
    "                if obj == \"noTime\":\n",
    "                    continue\n",
    "                # 3：sauter hasNewName et sub.lower() == obj.lower()\n",
    "                if rel == \"hasNewName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "                if rel == \"hasOldName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "\n",
    "                # garder ce triplet\n",
    "                new_triples.append(triple)\n",
    "\n",
    "            # actualiser les triplets\n",
    "            data[\"triples\"] = new_triples\n",
    "\n",
    "        # Chaque ligne de sortie reste un dictionnaire JSON.\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\" Tous les filtres ont été appliqués avec succès. Le fichier de sortie a été enregistré à l’emplacement suivant ：\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dbdf6",
   "metadata": {},
   "source": [
    "ex. 待处理的数据集: {\"id\": \"11001_ouverture\", \"sent\": \"rue georges berger || Ouverture || Décret du 10 avril 1867 (UP)\", \"triples\": [{\"sub\": \"Rue Georges Berger\", \"rel\": \"hasGeometryChangeOn\", \"obj\": \"1867-04-10\"}]} \n",
    "我需要做成的数据集是和bert pure的格式一致的\n",
    "* \"doc_key\": 原先数据集对应的id内容\n",
    "* \"dataset\": \"évolutions d'événements\"\n",
    "* \"sentences\":  \"sent\"被cambert-base AutoTokenizer后的内容\n",
    "* \"ner\": bert识别出来的每个实体的token起始位置以及他们的标签\n",
    "关于ner部分, 实体的features:\n",
    " * *给的原始句子中 (rue georges berger || Ouverture || ... ) || 之前的地点实体统一被标注为LandmarkType\n",
    "** \"1867-04-10\"或\"du 25 avril 1994\"有时间的时间统一被标注成 Time\n",
    "**|| Classement || 在两个竖线中间的事件类型统一标注成EventTpye\n",
    "* \"relations\": 两个实体各自的起始位置以及他们的关系类型(以下是我的数据集的所有关系类型:\"hasNameChangeOn\",\"isNumberedOn\",\"hasNewName\", \"isClassifiedOn\", \"hasOldName\", \"appearsOn\", \"hasGeometryChangeOn\")\n",
    "以上就是我对于输出的文件的要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00269cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1baf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dateparser\n",
      "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2.9.0.post0)\n",
      "Collecting pytz>=2024.2 (from dateparser)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2024.11.6)\n",
      "Collecting tzlocal>=0.2 (from dateparser)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
      "Collecting tzdata (from tzlocal>=0.2->dateparser)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, tzlocal, dateparser\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [tzlocal]\n",
      "   -------------------- ------------------- 2/4 [tzlocal]\n",
      "   -------------------- ------------------- 2/4 [tzlocal]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ---------------------------------------- 4/4 [dateparser]\n",
      "\n",
      "Successfully installed dateparser-1.2.2 pytz-2025.2 tzdata-2025.2 tzlocal-5.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dateparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86c37996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity [rue des Rondonneaux] not found in [rue du cange || Historique || Précédemment, rue des Trois Soeurs]\n",
      "Entity [voie U/20] not found in [rue fernand raynaud || Historique || Elle avait été provisoirement dénommée U/20]\n",
      "Entity [C13] not found in [rue du fouarre || Ouverture || Ouverte au commencement du XIIIe siècle]\n",
      "Entity [voie DE/20] not found in [rue francis picabia || Historique || Elle avait été provisoirement dénommée DE/20]\n",
      "Entity [voie AE/15] not found in [rue gaston de caillavet || Historique || Elle avait été provisoirement dénommée AE/15]\n",
      "Entity [voie Q/10] not found in [rue georg friedrich haendel || Historique || Elle avait été provisoirement dénommée Q/10]\n",
      "Entity [port de l'Hôtel de Ville] not found in [rue goethe || Historique || Précédemment, rue de Cadix]\n",
      "Entity [port de l'Hôtel de Ville] not found in [rue goethe || Historique || Précédemment, rue de Cadix]\n",
      "Entity [Impasse de Constantine] not found in [villa de guelma || Historique || Précédemment, impasse de Guelma (arr du 1er février 1877), ]\n",
      "Entity [place Charles de Gaulle] not found in [port de l'hôtel de ville || Historique || Précédemment, port des Ormes et port de l'Hôtel de Ville]\n",
      "Entity [voie C/1] not found in [place joachim du bellay || Historique || Elle avait été provisoirement dénommée C/1]\n",
      "Entity [voie AN/15] not found in [rue jongkind || Historique || Voie, provisoirement dénommée AN/15, créée dans le cadre de l'aménagement de la Zac `Saint-Charles`]\n",
      "Entity [voie D/13] not found in [cour du liégat || Historique || Elle avait été provisoirement dénommée D/13]\n",
      "Entity [rue Rouelle] not found in [rue louis blanc || Historique || Précédemment, rue de la Butte Chaumont (décision ministérielle du 2 octobre 1821)]\n",
      "Entity [voie Z/1] not found in [allée louis aragon || Historique || Elle avait été provisoirement dénommée Z/1]\n",
      "Entity [voie J/16] not found in [hameau nicolo || Historique || Il avait été provisoirement dénommé J/16]\n",
      "Entity [voie CH/19] not found in [terrasse du parc || Historique || Elle avait été provisoirement dénommée CH/19]\n",
      "[INFO] 模糊匹配 'cours des Petites Écuries' ≈ 'cour des petites écuries'，相似度=0.98\n",
      "Entity [rue Pajol] not found in [rue rémy de gourmont || Historique || Précédemment, rue Rémy et Jean de Gourmont]\n",
      "Entity [rue Pajol] not found in [rue rémy de gourmont || Historique || Précédemment, rue Rémy et Jean de Gourmont]\n",
      "Entity [rue de Bagnolet] not found in [rue des rondonneaux || Historique || Précédemment, rue des Audriettes et, plus anciennement, partie du sentier du Centre de la Cour des Noues]\n",
      "[INFO] 模糊匹配 'rue Rouvet' ≈ 'rue roue'，相似度=0.88\n",
      "[INFO] 模糊匹配 'rue Rouvet' ≈ 'rue roue'，相似度=0.88\n",
      "Entity [rue Goethe] not found in [rue rouvet || Historique || Précédemment, rue de Calais]\n",
      "Entity [rue Goethe] not found in [rue rouvet || Historique || Précédemment, rue de Calais]\n",
      "Entity [voie AZ/12] not found in [place rutebeuf || Historique || Elle avait été provisoirement dénommée AZ/12 lors de l'aménagement de la ZAC Chalon]\n",
      "[INFO] 模糊匹配 'Boulevard de la Chopinette' ≈ ' boulevards de la Chopinette'，相似度=0.98\n",
      "[INFO] 模糊匹配 'Boulevard de la Chopinette' ≈ ' boulevards de la Chopinette'，相似度=0.98\n",
      "Entity [Boulevard du Combat] not found in [boulevard de la villette || Dénomination || Arrêté du 30 décembre 1864, réunissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "Entity [Boulevard du Combat] not found in [boulevard de la villette || Dénomination || Arrêté du 30 décembre 1864, réunissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "Entity [Boulevard de la Butte Chaumont] not found in [boulevard de la villette || Dénomination || Arrêté du 30 décembre 1864, réunissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "Entity [Boulevard de la Butte Chaumont] not found in [boulevard de la villette || Dénomination || Arrêté du 30 décembre 1864, réunissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "[INFO] 模糊匹配 'Parvis-place Notre-Dame - Jean-Paul Ii' ≈ 'parvis, place notre-dame, jean-paul ii'，相似度=0.94\n",
      "[INFO] 模糊匹配 'Parvis-place Notre-Dame - Jean-Paul II' ≈ 'parvis, place notre-dame, jean-paul ii'，相似度=0.94\n",
      "[INFO] 模糊匹配 'Parvis-place Notre-Dame - Jean-Paul II' ≈ 'parvis, place notre-dame, jean-paul ii'，相似度=0.94\n",
      "[INFO] 模糊匹配 'Parvis-place Notre-Dame - Jean-Paul II' ≈ 'parvis, place notre-dame, jean-paul ii'，相似度=0.94\n",
      "[INFO] 模糊匹配 'Rue Neuve Notre-Dame' ≈ ' rues Neuve Notre-Dame'，相似度=0.97\n",
      "[INFO] 模糊匹配 'Rue Saint-Christophe' ≈ ' et Saint-Christophe'，相似度=0.92\n",
      "Entity [voie AW/20] not found in [rue albert willemetz || Historique || Elle avait été provisoirement dénommée AW/20]\n",
      "Entity [voie BF/12] not found in [rue albinoni || Historique || Elle avait été provisoirement dénommée BF/12]\n",
      "Entity [passage Charles Dallery] not found in [rue de campo-formio || Historique || Précédemment, petite rue d'Austerlitz, plus anciennement, rue des Etroites Ruelles]\n",
      "Entity [Boulevard du Palais] not found in [passage charles dallery || Historique || Précédemment, passage Vaucanson]\n",
      "Entity [rue de Campo-Formio] not found in [place charles de gaulle || Historique || Précédemment, place de l'Etoile]\n",
      "Entity [rue de Campo-Formio] not found in [place charles de gaulle || Historique || Précédemment, place de l'Etoile]\n",
      "[INFO] 模糊匹配 'Rue Vieille Notre-Dame' ≈ ' rues Vieille Notre-Dame'，相似度=0.98\n",
      "[INFO] 模糊匹配 'Rue du Pont aux Biches' ≈ ' et du Pont aux Biches'，相似度=0.91\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "import difflib\n",
    "import re\n",
    "# 你用的camembert-base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/jguo/Desktop/PURE-main/camembert-base\")\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"小写去变音去空格，方便宽松匹配。\"\"\"\n",
    "    return unidecode(text).lower().replace(\" \", \"\").replace(\"’\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "def find_token_span(tokens, text, entity, entity_type=None):\n",
    "    if entity_type == \"date\":\n",
    "        mois_fr = [\"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                   \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\"]\n",
    "        mois_fr_accent = [\"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                          \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\"]\n",
    "        # 兼容 YYYY-MM-DD / YYYY-MM / YYYY\n",
    "        m_full = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", entity)\n",
    "        m_month = re.fullmatch(r\"(\\d{4})-(\\d{2})\", entity)\n",
    "        m_year = re.fullmatch(r\"(\\d{4})\", entity)\n",
    "        year, month_idx, day = None, None, None\n",
    "\n",
    "        if m_full:\n",
    "            year = m_full.group(1)\n",
    "            month_idx = int(m_full.group(2)) - 1\n",
    "            day = int(m_full.group(3))\n",
    "        elif m_month:\n",
    "            year = m_month.group(1)\n",
    "            month_idx = int(m_month.group(2)) - 1\n",
    "        elif m_year:\n",
    "            year = m_year.group(1)\n",
    "\n",
    "        txt_norm = unidecode(text).lower()\n",
    "        candidates = []\n",
    "\n",
    "        # A. 带“1er”/数字/无日的月-年组合\n",
    "        if year and month_idx is not None:\n",
    "            for mois in [mois_fr[month_idx], mois_fr_accent[month_idx]]:\n",
    "                # 1. 精确日匹配 (\"3 fevrier 1877\"、\"03 fevrier 1877\"、\"3 février 1877\"、\"1er fevrier 1877\")\n",
    "                if day:\n",
    "                    # 支持1er和数字前缀\n",
    "                    regexs = [\n",
    "                        rf\"\\b{day}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b{str(day).zfill(2)}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b1er\\s+{mois}\\s+{year}\\b\" if day == 1 else \"\",\n",
    "                    ]\n",
    "                    for rgx in regexs:\n",
    "                        if not rgx: continue\n",
    "                        mobj = re.search(rgx, txt_norm)\n",
    "                        if mobj:\n",
    "                            candidates.append((mobj.start(), mobj.end()))\n",
    "                # 2. 宽松月-年（如 \"fevrier 1877\"）\n",
    "                rgx = rf\"\\b{mois}\\s+{year}\\b\"\n",
    "                mobj = re.search(rgx, txt_norm)\n",
    "                if mobj:\n",
    "                    candidates.append((mobj.start(), mobj.end()))\n",
    "        # B. 仅年份\n",
    "        elif year:\n",
    "            mobj = re.search(rf\"\\b{year}\\b\", txt_norm)\n",
    "            if mobj:\n",
    "                candidates.append((mobj.start(), mobj.end()))\n",
    "\n",
    "        # 按最先出现位置返回\n",
    "        if candidates:\n",
    "            candidates.sort()\n",
    "            char_start, char_end = candidates[0]\n",
    "            return char_to_token_span(tokens, unidecode(text), char_start, char_end)\n",
    "\n",
    "        # fallback: ISO日期或直接字符串\n",
    "        if entity in text:\n",
    "            char_start = text.index(entity)\n",
    "            return char_to_token_span(tokens, text, char_start, char_start + len(entity))\n",
    "\n",
    "        print(f\"[WARN] 日期实体 {entity} 无法在文本 [{text}] 匹配\")\n",
    "        return None, None\n",
    "\n",
    "    # ...后续你的原始代码不变...\n",
    "\n",
    "    # 2. 先尝试原文直接查找\n",
    "    idx_raw = text.find(entity)\n",
    "    if idx_raw != -1:\n",
    "        char_start = idx_raw\n",
    "        char_end = idx_raw + len(entity)\n",
    "        return char_to_token_span(tokens, text, char_start, char_end)\n",
    "\n",
    "    # 3. 宽松归一化后匹配\n",
    "    text_norm = normalize(text)\n",
    "    entity_norm = normalize(entity)\n",
    "    idx_norm = text_norm.find(entity_norm)\n",
    "    if idx_norm != -1:\n",
    "        # 滑窗法在原文text上找归一化后片段\n",
    "        best_start, best_end = None, None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + 1, min(len(text), start + len(entity) + 8) + 1):\n",
    "                frag = text[start:end]\n",
    "                if normalize(frag) == entity_norm:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None and best_end is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "        else:\n",
    "            print(f\"[WARN] 归一化后未能匹配到实体 '{entity}' in 原文 '{text}'\")\n",
    "            # 进入模糊滑窗\n",
    "\n",
    "    # 4. Fuzzy模糊滑窗查找（归一化后字符距离相似度>0.8的片段）\n",
    "    max_ratio = 0\n",
    "    best_start, best_end = None, None\n",
    "    for start in range(len(text)):\n",
    "        for end in range(start + max(2, len(entity) - 4), min(len(text), start + len(entity) + 8)):\n",
    "            frag = text[start:end]\n",
    "            frag_norm = normalize(frag)\n",
    "            if len(frag_norm) < 3:  # 过滤掉无意义的小片段\n",
    "                continue\n",
    "            ratio = difflib.SequenceMatcher(None, frag_norm, entity_norm).ratio()\n",
    "            if ratio > 0.82 and ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "                best_start, best_end = start, end\n",
    "    if best_start is not None and best_end is not None:\n",
    "        print(f\"[INFO] 模糊匹配 '{entity}' ≈ '{text[best_start:best_end]}'，相似度={max_ratio:.2f}\")\n",
    "        return char_to_token_span(tokens, text, best_start, best_end)\n",
    "\n",
    "    # 5. fallback: 事件类型直接查找\n",
    "    if entity in text:\n",
    "        char_start = text.index(entity)\n",
    "        return char_to_token_span(tokens, text, char_start, char_start + len(entity))\n",
    "\n",
    "    print(f\"Entity [{entity}] not found in [{text}]\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def char_to_token_span(tokens, text, char_start, char_end):\n",
    "    \"\"\"把字符区间转为token区间\"\"\"\n",
    "    curr_char = 0\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # CamemBERT token恢复方式\n",
    "        token_str = token.replace(\"▁\", \" \")\n",
    "        token_str = token_str.strip()\n",
    "        # 在text中查找token的位置\n",
    "        while curr_char < len(text) and text[curr_char].isspace():\n",
    "            curr_char += 1\n",
    "        token_begin = curr_char\n",
    "        token_end = curr_char + len(token_str)\n",
    "        if start_token_idx is None and token_begin <= char_start < token_end:\n",
    "            start_token_idx = idx\n",
    "        if token_begin < char_end <= token_end:\n",
    "            end_token_idx = idx\n",
    "        curr_char = token_end\n",
    "    if start_token_idx is not None and end_token_idx is not None:\n",
    "        return start_token_idx, end_token_idx\n",
    "    return None, None\n",
    "\n",
    "def tag_event_type(sent):\n",
    "    # || Ouverture || / || Classement || / || Dénomination || / ...\n",
    "    for e in [\"Ouverture\", \"Classement\", \"Dénomination\", \"Numérotation\", \"Historique\"]:\n",
    "        pattern = f\"|| {e} ||\"\n",
    "        idx = sent.find(pattern)\n",
    "        if idx != -1:\n",
    "            return e, sent.split(pattern)[0], idx, idx+len(pattern)\n",
    "    return None, None, -1, -1\n",
    "\n",
    "def extract_time(sent):\n",
    "    # 简单匹配日期\n",
    "    import re\n",
    "    m = re.search(r\"\\d{1,2} [a-zéû]+ \\d{4}|\\d{4}-\\d{2}-\\d{2}\", sent)\n",
    "    if m:\n",
    "        return m.group(), m.start(), m.end()\n",
    "    return None, -1, -1\n",
    "\n",
    "def build_ner(sent, tokens):\n",
    "    ner = []\n",
    "    # 地点实体（||前面的）：LandmarkType\n",
    "    event, before_event, event_start, event_end = tag_event_type(sent)\n",
    "    \n",
    "    if before_event is not None and before_event.strip():\n",
    "        loc_tokens = tokenizer.tokenize(before_event.strip())\n",
    "        ner.append([0, len(loc_tokens)-1, \"LandmarkType\"])\n",
    "\n",
    "    # 事件类型：EventType\n",
    "    if event and event_start > 0:\n",
    "        event_tokens = tokenizer.tokenize(event, )\n",
    "        # 找到event在分词中的位置\n",
    "        event_idx = sent.split().index(event)\n",
    "        ner.append([event_idx, event_idx+len(event_tokens)-1, \"EventType\"])\n",
    "    # 时间：Time\n",
    "    time_str, t_start, t_end = extract_time(sent)\n",
    "    if time_str:\n",
    "        # 找时间在token中的下标\n",
    "        sent_before_time = sent[:t_start]\n",
    "        before_tokens = tokenizer.tokenize(sent_before_time, )\n",
    "        time_tokens = tokenizer.tokenize(time_str, )\n",
    "        ner.append([len(before_tokens), len(before_tokens)+len(time_tokens)-1, \"Time\"])\n",
    "    return ner\n",
    "\n",
    "def looks_like_date(s):\n",
    "    \"\"\"\n",
    "    判断字符串是否为日期（兼容法语和ISO格式）\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    # ISO日期\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", s):\n",
    "        return True\n",
    "    # 法语日期：13 février 1911\n",
    "    if re.fullmatch(r\"\\d{1,2} [a-zéû]+ \\d{4}\", s.lower()):\n",
    "        return True\n",
    "    # 法语仅年份\n",
    "    if re.fullmatch(r\"\\d{4}\", s):\n",
    "        return True\n",
    "    # 其他可扩展\n",
    "    return False\n",
    "\n",
    "def build_relations(sent, tokens, ner, triples):\n",
    "    relations = []\n",
    "    for triple in triples:\n",
    "        sub_span = None\n",
    "        obj_span = None\n",
    "        sub_type = \"landmark\"\n",
    "        # 新日期判断\n",
    "        obj_type = \"date\" if looks_like_date(triple[\"obj\"]) else \"landmark\"\n",
    "        for i, (ent, label, ent_type) in enumerate([\n",
    "            (triple[\"sub\"], \"LandmarkType\", sub_type),\n",
    "            (triple[\"obj\"], \"Time\", obj_type)\n",
    "        ]):\n",
    "            span = find_token_span(tokens, sent, ent, entity_type=ent_type)\n",
    "            if span and span[0] is not None:\n",
    "                if i == 0:\n",
    "                    sub_span = span\n",
    "                else:\n",
    "                    obj_span = span\n",
    "        if sub_span is not None and obj_span is not None:\n",
    "            relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], triple[\"rel\"]])\n",
    "    return relations\n",
    "\n",
    "\n",
    "def process_one_line(d):\n",
    "    sent = d[\"sent\"]\n",
    "    doc_key = d[\"id\"]\n",
    "    tokens = tokenizer.tokenize(sent, )\n",
    "    ner = build_ner(sent, tokens)\n",
    "    relations = build_relations(sent, tokens, ner, d.get(\"triples\", []))\n",
    "    return {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"dataset\": \"évolutions d'événements\",\n",
    "        \"sentences\": [tokens],\n",
    "        \"ner\": [ner],\n",
    "        \"relations\": [relations]\n",
    "    }\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/train_pure.jsonl\"\n",
    "\n",
    "# 批量处理你的数据\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for data in dataset:\n",
    "        ex = process_one_line(data)\n",
    "        fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 正在处理第 1 条 / 97：10063_historique_1\n",
      "🔄 正在处理第 2 条 / 97：10328_dénomination\n",
      "🔄 正在处理第 3 条 / 97：10343_historique_5\n",
      "🔄 正在处理第 4 条 / 97：10474_historique_2\n",
      "🔄 正在处理第 5 条 / 97：10474_ouverture\n",
      "🔄 正在处理第 6 条 / 97：10602_ouverture\n",
      "🔄 正在处理第 7 条 / 97：10605_historique\n",
      "🔄 正在处理第 8 条 / 97：10908_dénomination\n",
      "🔄 正在处理第 9 条 / 97：11056_dénomination\n",
      "🔄 正在处理第 10 条 / 97：11076_historique_1\n",
      "🔄 正在处理第 11 条 / 97：11141_dénomination\n",
      "🔄 正在处理第 12 条 / 97：11150_dénomination\n",
      "🔄 正在处理第 13 条 / 97：11179_classement\n",
      "🔄 正在处理第 14 条 / 97：11179_historique\n",
      "🔄 正在处理第 15 条 / 97：11250_classement\n",
      "🔄 正在处理第 16 条 / 97：11270_ouverture\n",
      "🔄 正在处理第 17 条 / 97：11307_ouverture\n",
      "🔄 正在处理第 18 条 / 97：11592_dénomination\n",
      "🔄 正在处理第 19 条 / 97：11592_historique_3\n",
      "🔄 正在处理第 20 条 / 97：11598_numérotation\n",
      "🔄 正在处理第 21 条 / 97：11800_dénomination\n",
      "🔄 正在处理第 22 条 / 97：11800_historique\n",
      "🔄 正在处理第 23 条 / 97：11905_historique_2\n",
      "🔄 正在处理第 24 条 / 97：12075_dénomination_1\n",
      "🔄 正在处理第 25 条 / 97：12101_historique\n",
      "🔄 正在处理第 26 条 / 97：12122_ouverture\n",
      "🔄 正在处理第 27 条 / 97：12223_numérotation_1\n",
      "🔄 正在处理第 28 条 / 97：12374_historique_4\n",
      "🔄 正在处理第 29 条 / 97：12402_ouverture\n",
      "🔄 正在处理第 30 条 / 97：12440_historique\n",
      "🔄 正在处理第 31 条 / 97：12454_classement\n",
      "🔄 正在处理第 32 条 / 97：12521_numérotation_2\n",
      "🔄 正在处理第 33 条 / 97：12545_ouverture_1\n",
      "🔄 正在处理第 34 条 / 97：12635_numérotation_2\n",
      "🔄 正在处理第 35 条 / 97：12718_dénomination\n",
      "🔄 正在处理第 36 条 / 97：12781_classement_2\n",
      "🔄 正在处理第 37 条 / 97：12880_dénomination\n",
      "🔄 正在处理第 38 条 / 97：13023_historique_2\n",
      "🔄 正在处理第 39 条 / 97：13126_ouverture_2\n",
      "🔄 正在处理第 40 条 / 97：13158_historique\n",
      "🔄 正在处理第 41 条 / 97：13239_historique\n",
      "🔄 正在处理第 42 条 / 97：13252_numérotation\n",
      "🔄 正在处理第 43 条 / 97：13285_ouverture\n",
      "🔄 正在处理第 44 条 / 97：13325_ouverture\n",
      "🔄 正在处理第 45 条 / 97：13392_historique_1\n",
      "🔄 正在处理第 46 条 / 97：13433_dénomination\n",
      "🔄 正在处理第 47 条 / 97：13493_historique_3\n",
      "🔄 正在处理第 48 条 / 97：13558_classement_2\n",
      "🔄 正在处理第 49 条 / 97：13573_historique_2\n",
      "🔄 正在处理第 50 条 / 97：13807_dénomination\n",
      "🔄 正在处理第 51 条 / 97：13819_ouverture\n",
      "🔄 正在处理第 52 条 / 97：13855_ouverture\n",
      "🔄 正在处理第 53 条 / 97：13876_ouverture_4\n",
      "🔄 正在处理第 54 条 / 97：13889_historique_2\n",
      "🔄 正在处理第 55 条 / 97：13889_historique_3\n",
      "🔄 正在处理第 56 条 / 97：13889_numérotation\n",
      "🔄 正在处理第 57 条 / 97：14066_classement\n",
      "🔄 正在处理第 58 条 / 97：14066_ouverture\n",
      "🔄 正在处理第 59 条 / 97：14076_dénomination_1\n",
      "🔄 正在处理第 60 条 / 97：14076_historique_10\n",
      "🔄 正在处理第 61 条 / 97：14076_historique_5\n",
      "🔄 正在处理第 62 条 / 97：14076_historique_8\n",
      "🔄 正在处理第 63 条 / 97：14101_ouverture\n",
      "🔄 正在处理第 64 条 / 97：14107_ouverture\n",
      "🔄 正在处理第 65 条 / 97：14118_dénomination\n",
      "🔄 正在处理第 66 条 / 97：14134_historique_1\n",
      "🔄 正在处理第 67 条 / 97：14134_historique_3\n",
      "🔄 正在处理第 68 条 / 97：14873_historique_1\n",
      "🔄 正在处理第 69 条 / 97：14928_dénomination\n",
      "🔄 正在处理第 70 条 / 97：14954_historique\n",
      "🔄 正在处理第 71 条 / 97：14964_dénomination\n",
      "🔄 正在处理第 72 条 / 97：15062_historique\n",
      "🔄 正在处理第 73 条 / 97：15158_dénomination\n",
      "🔄 正在处理第 74 条 / 97：15307_dénomination\n",
      "🔄 正在处理第 75 条 / 97：15678_dénomination\n",
      "🔄 正在处理第 76 条 / 97：8784_historique_4\n",
      "🔄 正在处理第 77 条 / 97：8843_dénomination\n",
      "🔄 正在处理第 78 条 / 97：8843_ouverture\n",
      "🔄 正在处理第 79 条 / 97：8856_dénomination\n",
      "🔄 正在处理第 80 条 / 97：8898_classement_2\n",
      "🔄 正在处理第 81 条 / 97：8926_dénomination\n",
      "🔄 正在处理第 82 条 / 97：8946_dénomination\n",
      "🔄 正在处理第 83 条 / 97：9014_classement\n",
      "🔄 正在处理第 84 条 / 97：9014_numérotation\n",
      "🔄 正在处理第 85 条 / 97：9053_ouverture\n",
      "🔄 正在处理第 86 条 / 97：9064_historique_3\n",
      "🔄 正在处理第 87 条 / 97：9148_historique_4\n",
      "🔄 正在处理第 88 条 / 97：9390_dénomination\n",
      "🔄 正在处理第 89 条 / 97：9390_historique_3\n",
      "🔄 正在处理第 90 条 / 97：9408_historique\n",
      "🔄 正在处理第 91 条 / 97：9590_classement\n",
      "🔄 正在处理第 92 条 / 97：9757_classement\n",
      "🔄 正在处理第 93 条 / 97：9809_historique_2\n",
      "🔄 正在处理第 94 条 / 97：9845_historique_6\n",
      "🔄 正在处理第 95 条 / 97：9879_numérotation\n",
      "🔄 正在处理第 96 条 / 97：9907_historique_3\n",
      "🔄 正在处理第 97 条 / 97：13392_historique_2\n",
      "✅ CamemBERT 分词 + PURE 格式转换完成！共 97 条\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/jguo/Desktop/PURE-main/camembert-base\")\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def get_span_with_tokenizer(sentence, phrase, tokens, offsets):\n",
    "    \"\"\"\n",
    "    找 phrase 在 token list 中的 span （start_idx, end_idx）\n",
    "    \"\"\"\n",
    "    phrase = phrase.strip().lower()\n",
    "    for i, (start1, end1) in enumerate(offsets):\n",
    "        for j in range(i, len(offsets)):\n",
    "            start2, end2 = offsets[j]\n",
    "            span_text = sentence[start1:end2].lower().strip()\n",
    "            if phrase == span_text:\n",
    "                return i, j\n",
    "    return None\n",
    "\n",
    "def convert_entry(entry):\n",
    "    doc_key = entry[\"id\"]\n",
    "    sentence = entry[\"sent\"]\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "    ner = set()\n",
    "    relations = []\n",
    "\n",
    "    for triple in entry[\"triples\"]:\n",
    "        if isinstance(triple, dict):\n",
    "            sub, rel, obj = triple[\"sub\"], triple[\"rel\"], triple[\"obj\"]\n",
    "        else:\n",
    "            sub, rel, obj = triple\n",
    "\n",
    "        sub_span = get_span_with_tokenizer(sentence, sub, tokens, offsets)\n",
    "        obj_span = get_span_with_tokenizer(sentence, obj, tokens, offsets)\n",
    "\n",
    "        if sub_span:\n",
    "            ner.add((sub_span[0], sub_span[1], \"LANDMARK\"))\n",
    "        if obj_span:\n",
    "            ner.add((obj_span[0], obj_span[1], \"LANDMARK\"))\n",
    "        if sub_span and obj_span:\n",
    "            relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], rel])\n",
    "\n",
    "    return {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"dataset\": \"voies_paris\",\n",
    "        \"sentences\": [tokens],\n",
    "        \"ner\": list(ner),\n",
    "        \"relations\": relations\n",
    "    }\n",
    "\n",
    "def save_to_jsonl(data, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"/n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\"  # 你原始数据的路径\n",
    "    output_path = \"C:/Users/jguo/Desktop/PURE-main/data/train_pure.jsonl\"  # 输出 PURE 格式路径\n",
    "\n",
    "    data = load_dataset(input_path)\n",
    "    pure_data = []\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        print(f\"🔄 正在处理第 {i+1} 条 / {len(data)}：{entry['id']}\")\n",
    "        pure_data.append(convert_entry(entry))\n",
    "\n",
    "    save_to_jsonl(pure_data, output_path)\n",
    "\n",
    "    print(f\"✅ CamemBERT 分词 + PURE 格式转换完成！共 {len(pure_data)} 条\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
