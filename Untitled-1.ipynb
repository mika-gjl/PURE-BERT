{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b512fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "\n",
    "def extract_full_dates(text):\n",
    "    \"\"\"\n",
    "    æå– sent ä¸­æ‰€æœ‰å®Œæ•´æ—¥æœŸçŸ­è¯­ï¼ˆè¿”å›åŸæ–‡çŸ­è¯­åˆ—è¡¨ï¼Œæ³•è¯­æ ¼å¼ï¼‰\n",
    "    \"\"\"\n",
    "    # åŒ¹é… \"1er fÃ©vrier 1877\"ã€\"2 mars 1912\"ã€\"01 avril 1855\" ç­‰\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zÃ©Ã»]+(?:\\s+\\d{4})\"\n",
    "    results = []\n",
    "    for m in re.finditer(pattern, text, re.IGNORECASE):\n",
    "        phrase = text[m.start():m.end()]\n",
    "        # åªä¿ç•™èƒ½è¢« dateparser æ­£ç¡®è§£æä¸ºæ—¥æœŸçš„\n",
    "        dt = dateparser.parse(phrase, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            results.append(phrase.strip())\n",
    "    return results\n",
    "\n",
    "def replace_incomplete_dates(sent, triples):\n",
    "    # æå–å¥ä¸­æ‰€æœ‰å®Œæ•´æ—¥æœŸçŸ­è¯­ï¼ˆå¦‚â€œ1er fÃ©vrier 1877â€ï¼‰\n",
    "    full_dates = extract_full_dates(unidecode(sent))\n",
    "    # æ„å»ºæ˜ å°„è¡¨ { (å¹´,æœˆ): åŸå§‹çŸ­è¯­ }\n",
    "    mapping = {}\n",
    "    for d in full_dates:\n",
    "        dt = dateparser.parse(d, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            key = (str(dt.year), str(dt.month).zfill(2))\n",
    "            mapping[key] = d.strip()\n",
    "\n",
    "    # æ‰¹é‡å¤„ç†æ¯ä¸ª triple\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", obj)\n",
    "        if m:\n",
    "            year, month = m.group(1), m.group(2)\n",
    "            if (year, month) in mapping:\n",
    "                # æ›¿æ¢æˆå¥ä¸­çœŸå®æ—¶é—´çŸ­è¯­\n",
    "                triple[\"obj\"] = mapping[(year, month)]\n",
    "        # å¦‚éœ€å¤„ç† yyyy-mm-ddï¼Œå¯ç»§ç»­è¡¥å……\n",
    "        m2 = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", obj)\n",
    "        if m2:\n",
    "            year, month, day = m2.group(1), m2.group(2), int(m2.group(3))\n",
    "            for dstr, dphrase in mapping.items():\n",
    "                if dstr == (year, month):\n",
    "                    if str(day) in dphrase or f\"{day:02d}\" in dphrase or (day == 1 and \"1er\" in dphrase):\n",
    "                        triple[\"obj\"] = dphrase\n",
    "    return triples\n",
    "\n",
    "# ====== æ–‡ä»¶æ‰¹é‡å¤„ç† ======\n",
    "\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"    # è¾“å…¥æ–‡ä»¶è·¯å¾„\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"  # è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = replace_incomplete_dates(d[\"sent\"], d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eb25679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "def fr_date_to_iso(date_str):\n",
    "    dt = dateparser.parse(date_str, languages=['fr'])\n",
    "    if dt:\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return date_str\n",
    "\n",
    "def update_triples_date_obj(triples):\n",
    "    # å¯¹æ‰€æœ‰ triple çš„ objï¼Œè‹¥æ˜¯å®Œæ•´æ³•è¯­æ—¥æœŸçŸ­è¯­ï¼Œæ›¿æ¢ä¸ºISO\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zÃ©Ã»]+(?:\\s+\\d{4})\"\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        # åˆ¤æ–­æ˜¯ä¸æ˜¯æ³•è¯­æ—¥æœŸï¼ˆå¦‚â€œ1er fevrier 1877â€æˆ–â€œ21 mai 1956â€ç­‰ï¼‰\n",
    "        if re.fullmatch(pattern, obj, re.IGNORECASE):\n",
    "            iso_date = fr_date_to_iso(obj)\n",
    "            triple[\"obj\"] = iso_date\n",
    "    return triples\n",
    "\n",
    "# ========== æ‰¹é‡å¤„ç†JSONLæ–‡ä»¶ ==========\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = update_triples_date_obj(d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"å…¨éƒ¨å¤„ç†å®Œæ¯•ï¼\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "606801eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tous les filtres ont Ã©tÃ© appliquÃ©s avec succÃ¨s. Le fichier de sortie a Ã©tÃ© enregistrÃ© Ã  lâ€™emplacement suivant ï¼š C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "# prÃ©traitement du jeu de donÃ©es pour supprimer des triplets raisonnÃ©s\n",
    "#ex. Ã  supprimer: \"sub\": \"Boulevard de SÃ©bastopol\", \"rel\": \"isLandmarkType\", \"obj\": \"thoroughfare\" \n",
    "#\"sub\": \"Boulevard de SÃ©bastopol\", \"rel\": \"hasGeometryChangeOn\", \"obj\": \"noTime\"\n",
    "# si le sujet et l'objet entre la relation \"hasNewName\" sont pareils, on supprime le triplet. {\"sub\": \"Pont Alexandre III\", \"rel\": \"hasNewName\", \"obj\": \"pont Alexandre III\"}\n",
    "import json\n",
    "\n",
    "# changer le chemin d'accÃ¨s aux fichiers d'entrÃ©e et de sortie\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_train.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        if \"triples\" in data:\n",
    "            new_triples = []\n",
    "            for triple in data[\"triples\"]:\n",
    "                rel = triple.get(\"rel\", \"\")\n",
    "                obj = triple.get(\"obj\", \"\")\n",
    "                sub = triple.get(\"sub\", \"\")\n",
    "\n",
    "                # 1ï¼šsauter isLandmarkType\n",
    "                if rel == \"isLandmarkType\":\n",
    "                    continue\n",
    "                # 2ï¼šsauter obj == noTime\n",
    "                if obj == \"noTime\":\n",
    "                    continue\n",
    "                # 3ï¼šsauter hasNewName et sub.lower() == obj.lower()\n",
    "                if rel == \"hasNewName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "                if rel == \"hasOldName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "\n",
    "                # garder ce triplet\n",
    "                new_triples.append(triple)\n",
    "\n",
    "            # actualiser les triplets\n",
    "            data[\"triples\"] = new_triples\n",
    "\n",
    "        # Chaque ligne de sortie reste un dictionnaire JSON.\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\" Tous les filtres ont Ã©tÃ© appliquÃ©s avec succÃ¨s. Le fichier de sortie a Ã©tÃ© enregistrÃ© Ã  lâ€™emplacement suivant ï¼š\", output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dbdf6",
   "metadata": {},
   "source": [
    "ex. å¾…å¤„ç†çš„æ•°æ®é›†: {\"id\": \"11001_ouverture\", \"sent\": \"rue georges berger || Ouverture || DÃ©cret du 10 avril 1867 (UP)\", \"triples\": [{\"sub\": \"Rue Georges Berger\", \"rel\": \"hasGeometryChangeOn\", \"obj\": \"1867-04-10\"}]} \n",
    "æˆ‘éœ€è¦åšæˆçš„æ•°æ®é›†æ˜¯å’Œbert pureçš„æ ¼å¼ä¸€è‡´çš„\n",
    "* \"doc_key\": åŸå…ˆæ•°æ®é›†å¯¹åº”çš„idå†…å®¹\n",
    "* \"dataset\": \"Ã©volutions d'Ã©vÃ©nements\"\n",
    "* \"sentences\":  \"sent\"è¢«cambert-base AutoTokenizeråçš„å†…å®¹\n",
    "* \"ner\": bertè¯†åˆ«å‡ºæ¥çš„æ¯ä¸ªå®ä½“çš„tokenèµ·å§‹ä½ç½®ä»¥åŠä»–ä»¬çš„æ ‡ç­¾\n",
    "å…³äºneréƒ¨åˆ†, å®ä½“çš„features:\n",
    " * *ç»™çš„åŸå§‹å¥å­ä¸­ (rue georges berger || Ouverture || ... ) || ä¹‹å‰çš„åœ°ç‚¹å®ä½“ç»Ÿä¸€è¢«æ ‡æ³¨ä¸ºLandmarkType\n",
    "** \"1867-04-10\"æˆ–\"du 25 avril 1994\"æœ‰æ—¶é—´çš„æ—¶é—´ç»Ÿä¸€è¢«æ ‡æ³¨æˆ Time\n",
    "**|| Classement || åœ¨ä¸¤ä¸ªç«–çº¿ä¸­é—´çš„äº‹ä»¶ç±»å‹ç»Ÿä¸€æ ‡æ³¨æˆEventTpye\n",
    "* \"relations\": ä¸¤ä¸ªå®ä½“å„è‡ªçš„èµ·å§‹ä½ç½®ä»¥åŠä»–ä»¬çš„å…³ç³»ç±»å‹(ä»¥ä¸‹æ˜¯æˆ‘çš„æ•°æ®é›†çš„æ‰€æœ‰å…³ç³»ç±»å‹:\"hasNameChangeOn\",\"isNumberedOn\",\"hasNewName\", \"isClassifiedOn\", \"hasOldName\", \"appearsOn\", \"hasGeometryChangeOn\")\n",
    "ä»¥ä¸Šå°±æ˜¯æˆ‘å¯¹äºè¾“å‡ºçš„æ–‡ä»¶çš„è¦æ±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00269cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1baf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dateparser\n",
      "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2.9.0.post0)\n",
      "Collecting pytz>=2024.2 (from dateparser)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2024.11.6)\n",
      "Collecting tzlocal>=0.2 (from dateparser)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
      "Collecting tzdata (from tzlocal>=0.2->dateparser)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, tzlocal, dateparser\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [tzlocal]\n",
      "   -------------------- ------------------- 2/4 [tzlocal]\n",
      "   -------------------- ------------------- 2/4 [tzlocal]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ------------------------------ --------- 3/4 [dateparser]\n",
      "   ---------------------------------------- 4/4 [dateparser]\n",
      "\n",
      "Successfully installed dateparser-1.2.2 pytz-2025.2 tzdata-2025.2 tzlocal-5.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dateparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86c37996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity [rue des Rondonneaux] not found in [rue du cange || Historique || PrÃ©cÃ©demment, rue des Trois Soeurs]\n",
      "Entity [voie U/20] not found in [rue fernand raynaud || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e U/20]\n",
      "Entity [C13] not found in [rue du fouarre || Ouverture || Ouverte au commencement du XIIIe siÃ¨cle]\n",
      "Entity [voie DE/20] not found in [rue francis picabia || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e DE/20]\n",
      "Entity [voie AE/15] not found in [rue gaston de caillavet || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e AE/15]\n",
      "Entity [voie Q/10] not found in [rue georg friedrich haendel || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e Q/10]\n",
      "Entity [port de l'HÃ´tel de Ville] not found in [rue goethe || Historique || PrÃ©cÃ©demment, rue de Cadix]\n",
      "Entity [port de l'HÃ´tel de Ville] not found in [rue goethe || Historique || PrÃ©cÃ©demment, rue de Cadix]\n",
      "Entity [Impasse de Constantine] not found in [villa de guelma || Historique || PrÃ©cÃ©demment, impasse de Guelma (arr du 1er fÃ©vrier 1877), ]\n",
      "Entity [place Charles de Gaulle] not found in [port de l'hÃ´tel de ville || Historique || PrÃ©cÃ©demment, port des Ormes et port de l'HÃ´tel de Ville]\n",
      "Entity [voie C/1] not found in [place joachim du bellay || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e C/1]\n",
      "Entity [voie AN/15] not found in [rue jongkind || Historique || Voie, provisoirement dÃ©nommÃ©e AN/15, crÃ©Ã©e dans le cadre de l'amÃ©nagement de la Zac `Saint-Charles`]\n",
      "Entity [voie D/13] not found in [cour du liÃ©gat || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e D/13]\n",
      "Entity [rue Rouelle] not found in [rue louis blanc || Historique || PrÃ©cÃ©demment, rue de la Butte Chaumont (dÃ©cision ministÃ©rielle du 2 octobre 1821)]\n",
      "Entity [voie Z/1] not found in [allÃ©e louis aragon || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e Z/1]\n",
      "Entity [voie J/16] not found in [hameau nicolo || Historique || Il avait Ã©tÃ© provisoirement dÃ©nommÃ© J/16]\n",
      "Entity [voie CH/19] not found in [terrasse du parc || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e CH/19]\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'cours des Petites Ã‰curies' â‰ˆ 'cour des petites Ã©curies'ï¼Œç›¸ä¼¼åº¦=0.98\n",
      "Entity [rue Pajol] not found in [rue rÃ©my de gourmont || Historique || PrÃ©cÃ©demment, rue RÃ©my et Jean de Gourmont]\n",
      "Entity [rue Pajol] not found in [rue rÃ©my de gourmont || Historique || PrÃ©cÃ©demment, rue RÃ©my et Jean de Gourmont]\n",
      "Entity [rue de Bagnolet] not found in [rue des rondonneaux || Historique || PrÃ©cÃ©demment, rue des Audriettes et, plus anciennement, partie du sentier du Centre de la Cour des Noues]\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'rue Rouvet' â‰ˆ 'rue roue'ï¼Œç›¸ä¼¼åº¦=0.88\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'rue Rouvet' â‰ˆ 'rue roue'ï¼Œç›¸ä¼¼åº¦=0.88\n",
      "Entity [rue Goethe] not found in [rue rouvet || Historique || PrÃ©cÃ©demment, rue de Calais]\n",
      "Entity [rue Goethe] not found in [rue rouvet || Historique || PrÃ©cÃ©demment, rue de Calais]\n",
      "Entity [voie AZ/12] not found in [place rutebeuf || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e AZ/12 lors de l'amÃ©nagement de la ZAC Chalon]\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Boulevard de la Chopinette' â‰ˆ ' boulevards de la Chopinette'ï¼Œç›¸ä¼¼åº¦=0.98\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Boulevard de la Chopinette' â‰ˆ ' boulevards de la Chopinette'ï¼Œç›¸ä¼¼åº¦=0.98\n",
      "Entity [Boulevard du Combat] not found in [boulevard de la villette || DÃ©nomination || ArrÃªtÃ© du 30 dÃ©cembre 1864, rÃ©unissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "Entity [Boulevard du Combat] not found in [boulevard de la villette || DÃ©nomination || ArrÃªtÃ© du 30 dÃ©cembre 1864, rÃ©unissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "Entity [Boulevard de la Butte Chaumont] not found in [boulevard de la villette || DÃ©nomination || ArrÃªtÃ© du 30 dÃ©cembre 1864, rÃ©unissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "Entity [Boulevard de la Butte Chaumont] not found in [boulevard de la villette || DÃ©nomination || ArrÃªtÃ© du 30 dÃ©cembre 1864, rÃ©unissant les boulevards de la Chopinette, du Combat et de la Butte Chaumont au boulevard de la Villette]\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Parvis-place Notre-Dame - Jean-Paul Ii' â‰ˆ 'parvis, place notre-dame, jean-paul ii'ï¼Œç›¸ä¼¼åº¦=0.94\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Parvis-place Notre-Dame - Jean-Paul II' â‰ˆ 'parvis, place notre-dame, jean-paul ii'ï¼Œç›¸ä¼¼åº¦=0.94\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Parvis-place Notre-Dame - Jean-Paul II' â‰ˆ 'parvis, place notre-dame, jean-paul ii'ï¼Œç›¸ä¼¼åº¦=0.94\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Parvis-place Notre-Dame - Jean-Paul II' â‰ˆ 'parvis, place notre-dame, jean-paul ii'ï¼Œç›¸ä¼¼åº¦=0.94\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Rue Neuve Notre-Dame' â‰ˆ ' rues Neuve Notre-Dame'ï¼Œç›¸ä¼¼åº¦=0.97\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Rue Saint-Christophe' â‰ˆ ' et Saint-Christophe'ï¼Œç›¸ä¼¼åº¦=0.92\n",
      "Entity [voie AW/20] not found in [rue albert willemetz || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e AW/20]\n",
      "Entity [voie BF/12] not found in [rue albinoni || Historique || Elle avait Ã©tÃ© provisoirement dÃ©nommÃ©e BF/12]\n",
      "Entity [passage Charles Dallery] not found in [rue de campo-formio || Historique || PrÃ©cÃ©demment, petite rue d'Austerlitz, plus anciennement, rue des Etroites Ruelles]\n",
      "Entity [Boulevard du Palais] not found in [passage charles dallery || Historique || PrÃ©cÃ©demment, passage Vaucanson]\n",
      "Entity [rue de Campo-Formio] not found in [place charles de gaulle || Historique || PrÃ©cÃ©demment, place de l'Etoile]\n",
      "Entity [rue de Campo-Formio] not found in [place charles de gaulle || Historique || PrÃ©cÃ©demment, place de l'Etoile]\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Rue Vieille Notre-Dame' â‰ˆ ' rues Vieille Notre-Dame'ï¼Œç›¸ä¼¼åº¦=0.98\n",
      "[INFO] æ¨¡ç³ŠåŒ¹é… 'Rue du Pont aux Biches' â‰ˆ ' et du Pont aux Biches'ï¼Œç›¸ä¼¼åº¦=0.91\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "import difflib\n",
    "import re\n",
    "# ä½ ç”¨çš„camembert-base tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/jguo/Desktop/PURE-main/camembert-base\")\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"å°å†™å»å˜éŸ³å»ç©ºæ ¼ï¼Œæ–¹ä¾¿å®½æ¾åŒ¹é…ã€‚\"\"\"\n",
    "    return unidecode(text).lower().replace(\" \", \"\").replace(\"â€™\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "def find_token_span(tokens, text, entity, entity_type=None):\n",
    "    if entity_type == \"date\":\n",
    "        mois_fr = [\"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                   \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\"]\n",
    "        mois_fr_accent = [\"janvier\", \"fÃ©vrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                          \"aoÃ»t\", \"septembre\", \"octobre\", \"novembre\", \"dÃ©cembre\"]\n",
    "        # å…¼å®¹ YYYY-MM-DD / YYYY-MM / YYYY\n",
    "        m_full = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", entity)\n",
    "        m_month = re.fullmatch(r\"(\\d{4})-(\\d{2})\", entity)\n",
    "        m_year = re.fullmatch(r\"(\\d{4})\", entity)\n",
    "        year, month_idx, day = None, None, None\n",
    "\n",
    "        if m_full:\n",
    "            year = m_full.group(1)\n",
    "            month_idx = int(m_full.group(2)) - 1\n",
    "            day = int(m_full.group(3))\n",
    "        elif m_month:\n",
    "            year = m_month.group(1)\n",
    "            month_idx = int(m_month.group(2)) - 1\n",
    "        elif m_year:\n",
    "            year = m_year.group(1)\n",
    "\n",
    "        txt_norm = unidecode(text).lower()\n",
    "        candidates = []\n",
    "\n",
    "        # A. å¸¦â€œ1erâ€/æ•°å­—/æ— æ—¥çš„æœˆ-å¹´ç»„åˆ\n",
    "        if year and month_idx is not None:\n",
    "            for mois in [mois_fr[month_idx], mois_fr_accent[month_idx]]:\n",
    "                # 1. ç²¾ç¡®æ—¥åŒ¹é… (\"3 fevrier 1877\"ã€\"03 fevrier 1877\"ã€\"3 fÃ©vrier 1877\"ã€\"1er fevrier 1877\")\n",
    "                if day:\n",
    "                    # æ”¯æŒ1erå’Œæ•°å­—å‰ç¼€\n",
    "                    regexs = [\n",
    "                        rf\"\\b{day}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b{str(day).zfill(2)}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b1er\\s+{mois}\\s+{year}\\b\" if day == 1 else \"\",\n",
    "                    ]\n",
    "                    for rgx in regexs:\n",
    "                        if not rgx: continue\n",
    "                        mobj = re.search(rgx, txt_norm)\n",
    "                        if mobj:\n",
    "                            candidates.append((mobj.start(), mobj.end()))\n",
    "                # 2. å®½æ¾æœˆ-å¹´ï¼ˆå¦‚ \"fevrier 1877\"ï¼‰\n",
    "                rgx = rf\"\\b{mois}\\s+{year}\\b\"\n",
    "                mobj = re.search(rgx, txt_norm)\n",
    "                if mobj:\n",
    "                    candidates.append((mobj.start(), mobj.end()))\n",
    "        # B. ä»…å¹´ä»½\n",
    "        elif year:\n",
    "            mobj = re.search(rf\"\\b{year}\\b\", txt_norm)\n",
    "            if mobj:\n",
    "                candidates.append((mobj.start(), mobj.end()))\n",
    "\n",
    "        # æŒ‰æœ€å…ˆå‡ºç°ä½ç½®è¿”å›\n",
    "        if candidates:\n",
    "            candidates.sort()\n",
    "            char_start, char_end = candidates[0]\n",
    "            return char_to_token_span(tokens, unidecode(text), char_start, char_end)\n",
    "\n",
    "        # fallback: ISOæ—¥æœŸæˆ–ç›´æ¥å­—ç¬¦ä¸²\n",
    "        if entity in text:\n",
    "            char_start = text.index(entity)\n",
    "            return char_to_token_span(tokens, text, char_start, char_start + len(entity))\n",
    "\n",
    "        print(f\"[WARN] æ—¥æœŸå®ä½“ {entity} æ— æ³•åœ¨æ–‡æœ¬ [{text}] åŒ¹é…\")\n",
    "        return None, None\n",
    "\n",
    "    # ...åç»­ä½ çš„åŸå§‹ä»£ç ä¸å˜...\n",
    "\n",
    "    # 2. å…ˆå°è¯•åŸæ–‡ç›´æ¥æŸ¥æ‰¾\n",
    "    idx_raw = text.find(entity)\n",
    "    if idx_raw != -1:\n",
    "        char_start = idx_raw\n",
    "        char_end = idx_raw + len(entity)\n",
    "        return char_to_token_span(tokens, text, char_start, char_end)\n",
    "\n",
    "    # 3. å®½æ¾å½’ä¸€åŒ–ååŒ¹é…\n",
    "    text_norm = normalize(text)\n",
    "    entity_norm = normalize(entity)\n",
    "    idx_norm = text_norm.find(entity_norm)\n",
    "    if idx_norm != -1:\n",
    "        # æ»‘çª—æ³•åœ¨åŸæ–‡textä¸Šæ‰¾å½’ä¸€åŒ–åç‰‡æ®µ\n",
    "        best_start, best_end = None, None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + 1, min(len(text), start + len(entity) + 8) + 1):\n",
    "                frag = text[start:end]\n",
    "                if normalize(frag) == entity_norm:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None and best_end is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "        else:\n",
    "            print(f\"[WARN] å½’ä¸€åŒ–åæœªèƒ½åŒ¹é…åˆ°å®ä½“ '{entity}' in åŸæ–‡ '{text}'\")\n",
    "            # è¿›å…¥æ¨¡ç³Šæ»‘çª—\n",
    "\n",
    "    # 4. Fuzzyæ¨¡ç³Šæ»‘çª—æŸ¥æ‰¾ï¼ˆå½’ä¸€åŒ–åå­—ç¬¦è·ç¦»ç›¸ä¼¼åº¦>0.8çš„ç‰‡æ®µï¼‰\n",
    "    max_ratio = 0\n",
    "    best_start, best_end = None, None\n",
    "    for start in range(len(text)):\n",
    "        for end in range(start + max(2, len(entity) - 4), min(len(text), start + len(entity) + 8)):\n",
    "            frag = text[start:end]\n",
    "            frag_norm = normalize(frag)\n",
    "            if len(frag_norm) < 3:  # è¿‡æ»¤æ‰æ— æ„ä¹‰çš„å°ç‰‡æ®µ\n",
    "                continue\n",
    "            ratio = difflib.SequenceMatcher(None, frag_norm, entity_norm).ratio()\n",
    "            if ratio > 0.82 and ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "                best_start, best_end = start, end\n",
    "    if best_start is not None and best_end is not None:\n",
    "        print(f\"[INFO] æ¨¡ç³ŠåŒ¹é… '{entity}' â‰ˆ '{text[best_start:best_end]}'ï¼Œç›¸ä¼¼åº¦={max_ratio:.2f}\")\n",
    "        return char_to_token_span(tokens, text, best_start, best_end)\n",
    "\n",
    "    # 5. fallback: äº‹ä»¶ç±»å‹ç›´æ¥æŸ¥æ‰¾\n",
    "    if entity in text:\n",
    "        char_start = text.index(entity)\n",
    "        return char_to_token_span(tokens, text, char_start, char_start + len(entity))\n",
    "\n",
    "    print(f\"Entity [{entity}] not found in [{text}]\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def char_to_token_span(tokens, text, char_start, char_end):\n",
    "    \"\"\"æŠŠå­—ç¬¦åŒºé—´è½¬ä¸ºtokenåŒºé—´\"\"\"\n",
    "    curr_char = 0\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "    for idx, token in enumerate(tokens):\n",
    "        # CamemBERT tokenæ¢å¤æ–¹å¼\n",
    "        token_str = token.replace(\"â–\", \" \")\n",
    "        token_str = token_str.strip()\n",
    "        # åœ¨textä¸­æŸ¥æ‰¾tokençš„ä½ç½®\n",
    "        while curr_char < len(text) and text[curr_char].isspace():\n",
    "            curr_char += 1\n",
    "        token_begin = curr_char\n",
    "        token_end = curr_char + len(token_str)\n",
    "        if start_token_idx is None and token_begin <= char_start < token_end:\n",
    "            start_token_idx = idx\n",
    "        if token_begin < char_end <= token_end:\n",
    "            end_token_idx = idx\n",
    "        curr_char = token_end\n",
    "    if start_token_idx is not None and end_token_idx is not None:\n",
    "        return start_token_idx, end_token_idx\n",
    "    return None, None\n",
    "\n",
    "def tag_event_type(sent):\n",
    "    # || Ouverture || / || Classement || / || DÃ©nomination || / ...\n",
    "    for e in [\"Ouverture\", \"Classement\", \"DÃ©nomination\", \"NumÃ©rotation\", \"Historique\"]:\n",
    "        pattern = f\"|| {e} ||\"\n",
    "        idx = sent.find(pattern)\n",
    "        if idx != -1:\n",
    "            return e, sent.split(pattern)[0], idx, idx+len(pattern)\n",
    "    return None, None, -1, -1\n",
    "\n",
    "def extract_time(sent):\n",
    "    # ç®€å•åŒ¹é…æ—¥æœŸ\n",
    "    import re\n",
    "    m = re.search(r\"\\d{1,2} [a-zÃ©Ã»]+ \\d{4}|\\d{4}-\\d{2}-\\d{2}\", sent)\n",
    "    if m:\n",
    "        return m.group(), m.start(), m.end()\n",
    "    return None, -1, -1\n",
    "\n",
    "def build_ner(sent, tokens):\n",
    "    ner = []\n",
    "    # åœ°ç‚¹å®ä½“ï¼ˆ||å‰é¢çš„ï¼‰ï¼šLandmarkType\n",
    "    event, before_event, event_start, event_end = tag_event_type(sent)\n",
    "    \n",
    "    if before_event is not None and before_event.strip():\n",
    "        loc_tokens = tokenizer.tokenize(before_event.strip())\n",
    "        ner.append([0, len(loc_tokens)-1, \"LandmarkType\"])\n",
    "\n",
    "    # äº‹ä»¶ç±»å‹ï¼šEventType\n",
    "    if event and event_start > 0:\n",
    "        event_tokens = tokenizer.tokenize(event, )\n",
    "        # æ‰¾åˆ°eventåœ¨åˆ†è¯ä¸­çš„ä½ç½®\n",
    "        event_idx = sent.split().index(event)\n",
    "        ner.append([event_idx, event_idx+len(event_tokens)-1, \"EventType\"])\n",
    "    # æ—¶é—´ï¼šTime\n",
    "    time_str, t_start, t_end = extract_time(sent)\n",
    "    if time_str:\n",
    "        # æ‰¾æ—¶é—´åœ¨tokenä¸­çš„ä¸‹æ ‡\n",
    "        sent_before_time = sent[:t_start]\n",
    "        before_tokens = tokenizer.tokenize(sent_before_time, )\n",
    "        time_tokens = tokenizer.tokenize(time_str, )\n",
    "        ner.append([len(before_tokens), len(before_tokens)+len(time_tokens)-1, \"Time\"])\n",
    "    return ner\n",
    "\n",
    "def looks_like_date(s):\n",
    "    \"\"\"\n",
    "    åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºæ—¥æœŸï¼ˆå…¼å®¹æ³•è¯­å’ŒISOæ ¼å¼ï¼‰\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    # ISOæ—¥æœŸ\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", s):\n",
    "        return True\n",
    "    # æ³•è¯­æ—¥æœŸï¼š13 fÃ©vrier 1911\n",
    "    if re.fullmatch(r\"\\d{1,2} [a-zÃ©Ã»]+ \\d{4}\", s.lower()):\n",
    "        return True\n",
    "    # æ³•è¯­ä»…å¹´ä»½\n",
    "    if re.fullmatch(r\"\\d{4}\", s):\n",
    "        return True\n",
    "    # å…¶ä»–å¯æ‰©å±•\n",
    "    return False\n",
    "\n",
    "def build_relations(sent, tokens, ner, triples):\n",
    "    relations = []\n",
    "    for triple in triples:\n",
    "        sub_span = None\n",
    "        obj_span = None\n",
    "        sub_type = \"landmark\"\n",
    "        # æ–°æ—¥æœŸåˆ¤æ–­\n",
    "        obj_type = \"date\" if looks_like_date(triple[\"obj\"]) else \"landmark\"\n",
    "        for i, (ent, label, ent_type) in enumerate([\n",
    "            (triple[\"sub\"], \"LandmarkType\", sub_type),\n",
    "            (triple[\"obj\"], \"Time\", obj_type)\n",
    "        ]):\n",
    "            span = find_token_span(tokens, sent, ent, entity_type=ent_type)\n",
    "            if span and span[0] is not None:\n",
    "                if i == 0:\n",
    "                    sub_span = span\n",
    "                else:\n",
    "                    obj_span = span\n",
    "        if sub_span is not None and obj_span is not None:\n",
    "            relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], triple[\"rel\"]])\n",
    "    return relations\n",
    "\n",
    "\n",
    "def process_one_line(d):\n",
    "    sent = d[\"sent\"]\n",
    "    doc_key = d[\"id\"]\n",
    "    tokens = tokenizer.tokenize(sent, )\n",
    "    ner = build_ner(sent, tokens)\n",
    "    relations = build_relations(sent, tokens, ner, d.get(\"triples\", []))\n",
    "    return {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"dataset\": \"Ã©volutions d'Ã©vÃ©nements\",\n",
    "        \"sentences\": [tokens],\n",
    "        \"ner\": [ner],\n",
    "        \"relations\": [relations]\n",
    "    }\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/train_pure.jsonl\"\n",
    "\n",
    "# æ‰¹é‡å¤„ç†ä½ çš„æ•°æ®\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for data in dataset:\n",
    "        ex = process_one_line(data)\n",
    "        fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 1 æ¡ / 97ï¼š10063_historique_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 2 æ¡ / 97ï¼š10328_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 3 æ¡ / 97ï¼š10343_historique_5\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 4 æ¡ / 97ï¼š10474_historique_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 5 æ¡ / 97ï¼š10474_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 6 æ¡ / 97ï¼š10602_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 7 æ¡ / 97ï¼š10605_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 8 æ¡ / 97ï¼š10908_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 9 æ¡ / 97ï¼š11056_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 10 æ¡ / 97ï¼š11076_historique_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 11 æ¡ / 97ï¼š11141_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 12 æ¡ / 97ï¼š11150_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 13 æ¡ / 97ï¼š11179_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 14 æ¡ / 97ï¼š11179_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 15 æ¡ / 97ï¼š11250_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 16 æ¡ / 97ï¼š11270_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 17 æ¡ / 97ï¼š11307_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 18 æ¡ / 97ï¼š11592_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 19 æ¡ / 97ï¼š11592_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 20 æ¡ / 97ï¼š11598_numÃ©rotation\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 21 æ¡ / 97ï¼š11800_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 22 æ¡ / 97ï¼š11800_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 23 æ¡ / 97ï¼š11905_historique_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 24 æ¡ / 97ï¼š12075_dÃ©nomination_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 25 æ¡ / 97ï¼š12101_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 26 æ¡ / 97ï¼š12122_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 27 æ¡ / 97ï¼š12223_numÃ©rotation_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 28 æ¡ / 97ï¼š12374_historique_4\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 29 æ¡ / 97ï¼š12402_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 30 æ¡ / 97ï¼š12440_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 31 æ¡ / 97ï¼š12454_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 32 æ¡ / 97ï¼š12521_numÃ©rotation_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 33 æ¡ / 97ï¼š12545_ouverture_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 34 æ¡ / 97ï¼š12635_numÃ©rotation_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 35 æ¡ / 97ï¼š12718_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 36 æ¡ / 97ï¼š12781_classement_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 37 æ¡ / 97ï¼š12880_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 38 æ¡ / 97ï¼š13023_historique_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 39 æ¡ / 97ï¼š13126_ouverture_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 40 æ¡ / 97ï¼š13158_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 41 æ¡ / 97ï¼š13239_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 42 æ¡ / 97ï¼š13252_numÃ©rotation\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 43 æ¡ / 97ï¼š13285_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 44 æ¡ / 97ï¼š13325_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 45 æ¡ / 97ï¼š13392_historique_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 46 æ¡ / 97ï¼š13433_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 47 æ¡ / 97ï¼š13493_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 48 æ¡ / 97ï¼š13558_classement_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 49 æ¡ / 97ï¼š13573_historique_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 50 æ¡ / 97ï¼š13807_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 51 æ¡ / 97ï¼š13819_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 52 æ¡ / 97ï¼š13855_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 53 æ¡ / 97ï¼š13876_ouverture_4\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 54 æ¡ / 97ï¼š13889_historique_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 55 æ¡ / 97ï¼š13889_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 56 æ¡ / 97ï¼š13889_numÃ©rotation\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 57 æ¡ / 97ï¼š14066_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 58 æ¡ / 97ï¼š14066_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 59 æ¡ / 97ï¼š14076_dÃ©nomination_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 60 æ¡ / 97ï¼š14076_historique_10\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 61 æ¡ / 97ï¼š14076_historique_5\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 62 æ¡ / 97ï¼š14076_historique_8\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 63 æ¡ / 97ï¼š14101_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 64 æ¡ / 97ï¼š14107_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 65 æ¡ / 97ï¼š14118_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 66 æ¡ / 97ï¼š14134_historique_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 67 æ¡ / 97ï¼š14134_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 68 æ¡ / 97ï¼š14873_historique_1\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 69 æ¡ / 97ï¼š14928_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 70 æ¡ / 97ï¼š14954_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 71 æ¡ / 97ï¼š14964_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 72 æ¡ / 97ï¼š15062_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 73 æ¡ / 97ï¼š15158_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 74 æ¡ / 97ï¼š15307_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 75 æ¡ / 97ï¼š15678_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 76 æ¡ / 97ï¼š8784_historique_4\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 77 æ¡ / 97ï¼š8843_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 78 æ¡ / 97ï¼š8843_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 79 æ¡ / 97ï¼š8856_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 80 æ¡ / 97ï¼š8898_classement_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 81 æ¡ / 97ï¼š8926_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 82 æ¡ / 97ï¼š8946_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 83 æ¡ / 97ï¼š9014_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 84 æ¡ / 97ï¼š9014_numÃ©rotation\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 85 æ¡ / 97ï¼š9053_ouverture\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 86 æ¡ / 97ï¼š9064_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 87 æ¡ / 97ï¼š9148_historique_4\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 88 æ¡ / 97ï¼š9390_dÃ©nomination\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 89 æ¡ / 97ï¼š9390_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 90 æ¡ / 97ï¼š9408_historique\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 91 æ¡ / 97ï¼š9590_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 92 æ¡ / 97ï¼š9757_classement\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 93 æ¡ / 97ï¼š9809_historique_2\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 94 æ¡ / 97ï¼š9845_historique_6\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 95 æ¡ / 97ï¼š9879_numÃ©rotation\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 96 æ¡ / 97ï¼š9907_historique_3\n",
      "ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ 97 æ¡ / 97ï¼š13392_historique_2\n",
      "âœ… CamemBERT åˆ†è¯ + PURE æ ¼å¼è½¬æ¢å®Œæˆï¼å…± 97 æ¡\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/jguo/Desktop/PURE-main/camembert-base\")\n",
    "\n",
    "def load_dataset(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def get_span_with_tokenizer(sentence, phrase, tokens, offsets):\n",
    "    \"\"\"\n",
    "    æ‰¾ phrase åœ¨ token list ä¸­çš„ span ï¼ˆstart_idx, end_idxï¼‰\n",
    "    \"\"\"\n",
    "    phrase = phrase.strip().lower()\n",
    "    for i, (start1, end1) in enumerate(offsets):\n",
    "        for j in range(i, len(offsets)):\n",
    "            start2, end2 = offsets[j]\n",
    "            span_text = sentence[start1:end2].lower().strip()\n",
    "            if phrase == span_text:\n",
    "                return i, j\n",
    "    return None\n",
    "\n",
    "def convert_entry(entry):\n",
    "    doc_key = entry[\"id\"]\n",
    "    sentence = entry[\"sent\"]\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "    ner = set()\n",
    "    relations = []\n",
    "\n",
    "    for triple in entry[\"triples\"]:\n",
    "        if isinstance(triple, dict):\n",
    "            sub, rel, obj = triple[\"sub\"], triple[\"rel\"], triple[\"obj\"]\n",
    "        else:\n",
    "            sub, rel, obj = triple\n",
    "\n",
    "        sub_span = get_span_with_tokenizer(sentence, sub, tokens, offsets)\n",
    "        obj_span = get_span_with_tokenizer(sentence, obj, tokens, offsets)\n",
    "\n",
    "        if sub_span:\n",
    "            ner.add((sub_span[0], sub_span[1], \"LANDMARK\"))\n",
    "        if obj_span:\n",
    "            ner.add((obj_span[0], obj_span[1], \"LANDMARK\"))\n",
    "        if sub_span and obj_span:\n",
    "            relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], rel])\n",
    "\n",
    "    return {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"dataset\": \"voies_paris\",\n",
    "        \"sentences\": [tokens],\n",
    "        \"ner\": list(ner),\n",
    "        \"relations\": relations\n",
    "    }\n",
    "\n",
    "def save_to_jsonl(data, out_path):\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"/n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"C:/Users/jguo/Desktop/PURE-main/data/train_filtered.jsonl\"  # ä½ åŸå§‹æ•°æ®çš„è·¯å¾„\n",
    "    output_path = \"C:/Users/jguo/Desktop/PURE-main/data/train_pure.jsonl\"  # è¾“å‡º PURE æ ¼å¼è·¯å¾„\n",
    "\n",
    "    data = load_dataset(input_path)\n",
    "    pure_data = []\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        print(f\"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i+1} æ¡ / {len(data)}ï¼š{entry['id']}\")\n",
    "        pure_data.append(convert_entry(entry))\n",
    "\n",
    "    save_to_jsonl(pure_data, output_path)\n",
    "\n",
    "    print(f\"âœ… CamemBERT åˆ†è¯ + PURE æ ¼å¼è½¬æ¢å®Œæˆï¼å…± {len(pure_data)} æ¡\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
