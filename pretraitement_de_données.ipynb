{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2cd25b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered_landmarktype.jsonl\"\n",
    "\n",
    "def fix_landmark_type(triples):\n",
    "    new_triples = []\n",
    "    for triple in triples:\n",
    "        if triple.get(\"rel\") == \"isLandmarkType\":\n",
    "            # 取地名类型前缀（如 \"Rue\"）\n",
    "            sub = triple.get(\"sub\", \"\")\n",
    "            prefix = sub.split()[0] if sub else \"\"\n",
    "            # 新 triple\n",
    "            new_triples.append({\n",
    "                \"sub\": prefix,\n",
    "                \"rel\": \"isLandmarkTypeOf\",\n",
    "                \"obj\": sub.lower()\n",
    "            })\n",
    "        else:\n",
    "            new_triples.append(triple)\n",
    "    return new_triples\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        if \"triples\" in data:\n",
    "            data[\"triples\"] = fix_landmark_type(data[\"triples\"])\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b512fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "\n",
    "def extract_full_dates(text):\n",
    "    \"\"\"\n",
    "    Extraire toutes les expressions de dates complètes dans la phrase (retourne une liste de segments d'origine, format français)\n",
    "    \"\"\"\n",
    "    # Correspond à \"1er février 1877\", \"2 mars 1912\", \"01 avril 1855\", etc.\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    results = []\n",
    "    for m in re.finditer(pattern, text, re.IGNORECASE):\n",
    "        phrase = text[m.start():m.end()]\n",
    "        # On ne garde que celles qui peuvent être correctement analysées par dateparser\n",
    "        dt = dateparser.parse(phrase, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            results.append(phrase.strip())\n",
    "    return results\n",
    "\n",
    "def replace_incomplete_dates(sent, triples):\n",
    "    # Extraire toutes les dates complètes dans la phrase (ex : « 1er février 1877 »)\n",
    "    full_dates = extract_full_dates(unidecode(sent))\n",
    "    # Construire une table de correspondance { (année, mois): expression d'origine }\n",
    "    mapping = {}\n",
    "    for d in full_dates:\n",
    "        dt = dateparser.parse(d, languages=[\"fr\"])\n",
    "        if dt:\n",
    "            key = (str(dt.year), str(dt.month).zfill(2))\n",
    "            mapping[key] = d.strip()\n",
    "\n",
    "    # Traiter chaque triple individuellement\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        m = re.fullmatch(r\"(\\d{4})-(\\d{2})\", obj)\n",
    "        if m:\n",
    "            year, month = m.group(1), m.group(2)\n",
    "            if (year, month) in mapping:\n",
    "                # Remplacer par l'expression temporelle réelle trouvée dans la phrase\n",
    "                triple[\"obj\"] = mapping[(year, month)]\n",
    "        # Pour traiter aussi les formats yyyy-mm-dd si besoin\n",
    "        m2 = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", obj)\n",
    "        if m2:\n",
    "            year, month, day = m2.group(1), m2.group(2), int(m2.group(3))\n",
    "            for dstr, dphrase in mapping.items():\n",
    "                if dstr == (year, month):\n",
    "                    if str(day) in dphrase or f\"{day:02d}\" in dphrase or (day == 1 and \"1er\" in dphrase):\n",
    "                        triple[\"obj\"] = dphrase\n",
    "    return triples\n",
    "\n",
    "# ====== Traitement batch du fichier ======\n",
    "\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered_landmarktype.jsonl\"    # Chemin du fichier d'entrée\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"  # Chemin du fichier de sortie\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = replace_incomplete_dates(d[\"sent\"], d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7eb25679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé !\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import dateparser\n",
    "import re\n",
    "\n",
    "def fr_date_to_iso(date_str):\n",
    "    # Convertit une date en français (« 1er février 1877 », etc.) au format ISO (« 1877-02-01 »)\n",
    "    dt = dateparser.parse(date_str, languages=['fr'])\n",
    "    if dt:\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    return date_str\n",
    "\n",
    "def update_triples_date_obj(triples):\n",
    "    # Pour chaque triple, si l'objet est une date complète en français, on la remplace par le format ISO\n",
    "    pattern = r\"\\b(?:1er|\\d{1,2})\\s+[a-zéû]+(?:\\s+\\d{4})\"\n",
    "    for triple in triples:\n",
    "        obj = triple.get(\"obj\", \"\")\n",
    "        # Vérifie si l'objet est une date en français (« 1er février 1877 », « 21 mai 1956 », etc.)\n",
    "        if re.fullmatch(pattern, obj, re.IGNORECASE):\n",
    "            iso_date = fr_date_to_iso(obj)\n",
    "            triple[\"obj\"] = iso_date\n",
    "    return triples\n",
    "\n",
    "# ========== Traitement batch du fichier JSONL ==========\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_fixed.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        d = json.loads(line)\n",
    "        d[\"triples\"] = update_triples_date_obj(d[\"triples\"])\n",
    "        fout.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Traitement terminé !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "606801eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tous les filtres ont été appliqués avec succès. Le fichier de sortie a été enregistré à l’emplacement suivant ： C:/Users/jguo/Desktop/PURE-main/data/test_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "# prétraitement du jeu de donées pour supprimer des triplets raisonnés\n",
    "#\"sub\": \"Boulevard de Sébastopol\", \"rel\": \"hasGeometryChangeOn\", \"obj\": \"noTime\"\n",
    "# si le sujet et l'objet entre la relation \"hasNewName\" sont pareils, on supprime le triplet. {\"sub\": \"Pont Alexandre III\", \"rel\": \"hasNewName\", \"obj\": \"pont Alexandre III\"}\n",
    "import json\n",
    "\n",
    "# changer le chemin d'accès aux fichiers d'entrée et de sortie\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/new_test.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered.jsonl\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        if \"triples\" in data:\n",
    "            new_triples = []\n",
    "            for triple in data[\"triples\"]:\n",
    "                rel = triple.get(\"rel\", \"\")\n",
    "                obj = triple.get(\"obj\", \"\")\n",
    "                sub = triple.get(\"sub\", \"\")\n",
    "\n",
    "                # 1：sauter obj == noTime\n",
    "                if obj == \"noTime\":\n",
    "                    continue\n",
    "                # 2：sauter hasNewName et sub.lower() == obj.lower()\n",
    "                if rel == \"hasNewName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "                if rel == \"hasOldName\" and sub.strip().lower() == obj.strip().lower():\n",
    "                    continue\n",
    "\n",
    "                # garder ce triplet\n",
    "                new_triples.append(triple)\n",
    "\n",
    "            # actualiser les triplets\n",
    "            data[\"triples\"] = new_triples\n",
    "\n",
    "        # Chaque ligne de sortie reste un dictionnaire JSON.\n",
    "        fout.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\" Tous les filtres ont été appliqués avec succès. Le fichier de sortie a été enregistré à l’emplacement suivant ：\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a47c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00269cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa1baf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dateparser in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (2024.11.6)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from dateparser) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\jguo\\desktop\\pure-main\\venv310\\lib\\site-packages (from tzlocal>=0.2->dateparser) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install dateparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa47d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from unidecode import unidecode\n",
    "import dateparser\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/Users/jguo/Desktop/PURE-main/camembert-base\")\n",
    "\n",
    "def normalize(text):\n",
    "    return unidecode(text).lower().replace(\" \", \"\").replace(\"’\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "def find_token_span(tokens, text, entity, entity_type=None):\n",
    "    # Sauter les entités \"thoroughfare\" et \"municipality\" sans aucun affichage\n",
    "    if entity_type in (\"thoroughfare\", \"municipality\"):\n",
    "        return None, None\n",
    "    # Appariement souple pour les codes de voies : \"voie AH/15\", \"AH/15\", etc.\n",
    "    if entity_type == \"landmark\":\n",
    "        code_pattern = r\"[A-Z]{1,3}/\\d{1,3}\"\n",
    "        entity_code = re.findall(code_pattern, entity)\n",
    "        if entity_code:\n",
    "            c = entity_code[0]\n",
    "            idx = text.find(c)\n",
    "            if idx != -1:\n",
    "                return char_to_token_span(tokens, text, idx, idx + len(c))\n",
    "        entity_words = entity.strip().split()\n",
    "        if len(entity_words) >= 2:\n",
    "            last_words = \" \".join(entity_words[-2:])\n",
    "            idx = text.lower().find(last_words.lower())\n",
    "            if idx != -1:\n",
    "                return char_to_token_span(tokens, text, idx, idx + len(last_words))\n",
    "            last_word = entity_words[-1]\n",
    "            idx2 = text.lower().find(last_word.lower())\n",
    "            if idx2 != -1:\n",
    "                left = max(0, idx2-6)\n",
    "                match = re.search(r\"(du|de|des|la|le)\\s+\" + re.escape(last_word), text[left:idx2+len(last_word)], re.IGNORECASE)\n",
    "                if match:\n",
    "                    span_start = left + match.start()\n",
    "                    span_end = left + match.end()\n",
    "                    return char_to_token_span(tokens, text, span_start, span_end)\n",
    "    if entity_type == \"date\":\n",
    "        century_match = re.fullmatch(r\"C(\\d{1,2})(e)?\", entity, re.IGNORECASE)\n",
    "        if century_match:\n",
    "            century_num = int(century_match.group(1))\n",
    "            roman_map = {\n",
    "                1: \"I\", 2: \"II\", 3: \"III\", 4: \"IV\", 5: \"V\", 6: \"VI\", 7: \"VII\", 8: \"VIII\", 9: \"IX\", 10: \"X\",\n",
    "                11: \"XI\", 12: \"XII\", 13: \"XIII\", 14: \"XIV\", 15: \"XV\", 16: \"XVI\", 17: \"XVII\", 18: \"XVIII\", 19: \"XIX\", 20: \"XX\"\n",
    "            }\n",
    "            roman = roman_map.get(century_num, \"\")\n",
    "            candidates = []\n",
    "            patterns = [\n",
    "                rf\"{century_num}e siècle\",\n",
    "                rf\"{century_num} siècle\",\n",
    "                rf\"{roman}e siècle\",\n",
    "                rf\"{roman} siècle\"\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                idx = text.lower().find(pat.lower())\n",
    "                if idx != -1:\n",
    "                    candidates.append((idx, idx + len(pat)))\n",
    "            if candidates:\n",
    "                char_start, char_end = candidates[0]\n",
    "                return char_to_token_span(tokens, text, char_start, char_end)\n",
    "        mois_fr = [\"janvier\", \"fevrier\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                   \"aout\", \"septembre\", \"octobre\", \"novembre\", \"decembre\"]\n",
    "        mois_fr_accent = [\"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\",\n",
    "                          \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\"]\n",
    "        m_full = re.fullmatch(r\"(\\d{4})-(\\d{2})-(\\d{2})\", entity)\n",
    "        m_month = re.fullmatch(r\"(\\d{4})-(\\d{2})\", entity)\n",
    "        m_year = re.fullmatch(r\"(\\d{4})\", entity)\n",
    "        year, month_idx, day = None, None, None\n",
    "        if m_full:\n",
    "            year = m_full.group(1)\n",
    "            month_idx = int(m_full.group(2)) - 1\n",
    "            day = int(m_full.group(3))\n",
    "        elif m_month:\n",
    "            year = m_month.group(1)\n",
    "            month_idx = int(m_month.group(2)) - 1\n",
    "        elif m_year:\n",
    "            year = m_year.group(1)\n",
    "        txt_norm = unidecode(text).lower()\n",
    "        candidates = []\n",
    "        if year and month_idx is not None:\n",
    "            for mois in [mois_fr[month_idx], mois_fr_accent[month_idx]]:\n",
    "                if day:\n",
    "                    regexs = [\n",
    "                        rf\"\\b{day}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b{str(day).zfill(2)}\\s+{mois}\\s+{year}\\b\",\n",
    "                        rf\"\\b1er\\s+{mois}\\s+{year}\\b\" if day == 1 else \"\",\n",
    "                    ]\n",
    "                    for rgx in regexs:\n",
    "                        if not rgx: continue\n",
    "                        mobj = re.search(rgx, txt_norm)\n",
    "                        if mobj:\n",
    "                            candidates.append((mobj.start(), mobj.end()))\n",
    "                rgx = rf\"\\b{mois}\\s+{year}\\b\"\n",
    "                mobj = re.search(rgx, txt_norm)\n",
    "                if mobj:\n",
    "                    candidates.append((mobj.start(), mobj.end()))\n",
    "        elif year:\n",
    "            mobj = re.search(rf\"\\b{year}\\b\", txt_norm)\n",
    "            if mobj:\n",
    "                candidates.append((mobj.start(), mobj.end()))\n",
    "        if candidates:\n",
    "            candidates.sort()\n",
    "            char_start, char_end = candidates[0]\n",
    "            return char_to_token_span(tokens, unidecode(text), char_start, char_end)\n",
    "        if entity in text:\n",
    "            char_start = text.index(entity)\n",
    "            return char_to_token_span(tokens, text, char_start, char_start + len(entity))\n",
    "        if entity_type is not None and entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "            print(f\"Entity [{entity}] not found in [{text}]\")\n",
    "        return None, None\n",
    "\n",
    "    idx_raw = text.find(entity)\n",
    "    if idx_raw != -1:\n",
    "        char_start = idx_raw\n",
    "        char_end = idx_raw + len(entity)\n",
    "        return char_to_token_span(tokens, text, char_start, char_end)\n",
    "    entity_norm = normalize(entity)\n",
    "    text_norm = normalize(text)\n",
    "    if entity_norm in text_norm:\n",
    "        idx = text_norm.index(entity_norm)\n",
    "        best_start, best_end = None, None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + len(entity), min(len(text), start + len(entity) + 8)):\n",
    "                frag = text[start:end]\n",
    "                if normalize(frag) == entity_norm:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None and best_end is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "    text_norm = normalize(text)\n",
    "    entity_norm = normalize(entity)\n",
    "    idx_norm = text_norm.find(entity_norm)\n",
    "    if idx_norm != -1:\n",
    "        best_start, best_end = None, None\n",
    "        for start in range(len(text)):\n",
    "            for end in range(start + 1, min(len(text), start + len(entity) + 8) + 1):\n",
    "                frag = text[start:end]\n",
    "                if normalize(frag) == entity_norm:\n",
    "                    best_start, best_end = start, end\n",
    "                    break\n",
    "            if best_start is not None:\n",
    "                break\n",
    "        if best_start is not None and best_end is not None:\n",
    "            return char_to_token_span(tokens, text, best_start, best_end)\n",
    "        else:\n",
    "            if entity_type is not None and entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "                print(f\"[AVERTISSEMENT] L'entité normalisée « {entity} » n'a pas été retrouvée dans le texte original « {text} ».\")\n",
    "    max_ratio = 0\n",
    "    best_start, best_end = None, None\n",
    "    for start in range(len(text)):\n",
    "        for end in range(start + max(2, len(entity) - 4), min(len(text), start + len(entity) + 8)):\n",
    "            frag = text[start:end]\n",
    "            frag_norm = normalize(frag)\n",
    "            if len(frag_norm) < 3:\n",
    "                continue\n",
    "            ratio = difflib.SequenceMatcher(None, frag_norm, entity_norm).ratio()\n",
    "            if ratio > 0.82 and ratio > max_ratio:\n",
    "                max_ratio = ratio\n",
    "                best_start, best_end = start, end\n",
    "    if best_start is not None and best_end is not None:\n",
    "        if entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "            print(f\"[INFO] Appariement flou '{entity}' ≈ '{text[best_start:best_end]}', similarité={max_ratio:.2f}\")\n",
    "        return char_to_token_span(tokens, text, best_start, best_end)\n",
    "    if entity_type is not None and entity_type not in (\"thoroughfare\", \"municipality\"):\n",
    "           print(f\"Entity [{entity}] not found in [{text}]\")\n",
    "    return None, None\n",
    "\n",
    "def char_to_token_span(tokens, text, char_start, char_end):\n",
    "    curr_char = 0\n",
    "    start_token_idx = None\n",
    "    end_token_idx = None\n",
    "    for idx, token in enumerate(tokens):\n",
    "        token_str = token.replace(\"▁\", \" \")\n",
    "        token_str = token_str.strip()\n",
    "        while curr_char < len(text) and text[curr_char].isspace():\n",
    "            curr_char += 1\n",
    "        token_begin = curr_char\n",
    "        token_end = curr_char + len(token_str)\n",
    "        if start_token_idx is None and token_begin <= char_start < token_end:\n",
    "            start_token_idx = idx\n",
    "        if token_begin < char_end <= token_end:\n",
    "            end_token_idx = idx\n",
    "        curr_char = token_end\n",
    "    if start_token_idx is not None and end_token_idx is not None:\n",
    "        return start_token_idx, end_token_idx\n",
    "    return None, None\n",
    "\n",
    "def find_sublist(lst, sub):\n",
    "    \"\"\"Trouve le début et la fin d'une sous-liste dans une liste.\"\"\"\n",
    "    for i in range(len(lst) - len(sub) + 1):\n",
    "        if lst[i:i+len(sub)] == sub:\n",
    "            return i, i + len(sub) - 1\n",
    "    return -1, -1\n",
    "\n",
    "def extract_time(sent):\n",
    "    # Recherche d'expressions de dates simples dans la phrase\n",
    "    import re\n",
    "    m = re.search(r\"\\d{1,2} [a-zéû]+ \\d{4}|\\d{4}-\\d{2}-\\d{2}\", sent)\n",
    "    if m:\n",
    "        return m.group(), m.start(), m.end()\n",
    "    return None, -1, -1\n",
    "\n",
    "def tag_event_type(sent):\n",
    "    import re\n",
    "    m = re.search(r\"\\|\\|\\s*([^\\|]+?)\\s*\\|\\|\", sent)\n",
    "    if m:\n",
    "        event = m.group(1).strip()\n",
    "        before_event = sent[:m.start()]\n",
    "        return event, before_event, m.start(), m.end()\n",
    "    return None, None, -1, -1\n",
    "\n",
    "def extract_time(sent):\n",
    "    m = re.search(r\"\\d{1,2} [a-zéû]+ \\d{4}|\\d{4}-\\d{2}-\\d{2}|\\d{4}|C\\d{1,2}(e)?\", sent)\n",
    "    if m:\n",
    "        return m.group(), m.start(), m.end()\n",
    "    return None, -1, -1\n",
    "\n",
    "def build_ner(sent, tokens, triples=None):\n",
    "    ner = []\n",
    "    geo_prefixes = [\"Rue\", \"Place\", \"Boulevard\", \"Quai\", \"Impasse\", \"Passage\", \"Allée\", \"Avenue\", \"Cours\", \"Chemin\"]\n",
    "    if triples:\n",
    "        for triple in triples:\n",
    "            if triple[\"rel\"] == \"isLandmarkTypeOf\" or triple[\"rel\"] in (\"hasOldName\", \"hasNewName\"):\n",
    "                loc_text = triple[\"obj\"]\n",
    "                loc_tokens = tokenizer.tokenize(loc_text)\n",
    "                start, end = find_sublist(tokens, loc_tokens)\n",
    "                if start == -1:\n",
    "                    continue\n",
    "                found = False\n",
    "                for prefix in geo_prefixes:\n",
    "                    prefix_tokens = tokenizer.tokenize(prefix)\n",
    "                    for i in range(len(loc_tokens) - len(prefix_tokens) + 1):\n",
    "                        if [t.lower() for t in loc_tokens[i:i+len(prefix_tokens)]] == [t.lower() for t in prefix_tokens]:\n",
    "                            ner.append([start+i, start+i+len(prefix_tokens)-1, \"caractéristique géographique\"])\n",
    "                            if i+len(prefix_tokens) < len(loc_tokens):\n",
    "                                ner.append([start+i+len(prefix_tokens), end, \"nom\"])\n",
    "                            ner.append([start, end, \"nom géographique\"])\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found:\n",
    "                        break\n",
    "                if not found:\n",
    "                    ner.append([start, end, \"nom géographique\"])\n",
    "                    ner.append([end, end, \"nom\"])\n",
    "    # 事件类型和时间的标注保持不变\n",
    "    event, before_event, event_start, event_end = tag_event_type(sent)\n",
    "    if event and event_start > 0:\n",
    "        event_tokens = tokenizer.tokenize(event)\n",
    "        start, end = find_sublist(tokens, event_tokens)\n",
    "        if start != -1:\n",
    "            ner.append([start, end, \"EventType\"])\n",
    "    time_str, t_start, t_end = extract_time(sent)\n",
    "    if time_str:\n",
    "        sent_before_time = sent[:t_start]\n",
    "        before_tokens = tokenizer.tokenize(sent_before_time, )\n",
    "        time_tokens = tokenizer.tokenize(time_str, )\n",
    "        ner.append([len(before_tokens), len(before_tokens)+len(time_tokens)-1, \"Time\"])\n",
    "    return ner\n",
    "\n",
    "def looks_like_date(s):\n",
    "    \"\"\"\n",
    "    Détermine si la chaîne correspond à un format de date (français, ISO ou siècle).\n",
    "\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    # ISO日期\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}\", s):\n",
    "        return True\n",
    "    # 法语日期：13 février 1911\n",
    "    if re.fullmatch(r\"\\d{1,2} [a-zéû]+ \\d{4}\", s.lower()):\n",
    "        return True\n",
    "    # 法语仅年份\n",
    "    if re.fullmatch(r\"\\d{4}\", s):\n",
    "        return True\n",
    "    # 世纪表达 C13 或 C13e\n",
    "    if re.fullmatch(r\"C\\d{1,2}(e)?\", s, re.IGNORECASE):\n",
    "        return True\n",
    "    # 其他可扩展\n",
    "    return False\n",
    "\n",
    "def build_relations(sent, tokens, ner, triples):\n",
    "    relations = []\n",
    "    def get_span_from_ner(entity, label):\n",
    "        # 优先返回 tokens拼接后和entity完全一致的区间\n",
    "        best_span = None\n",
    "        for span in ner:\n",
    "            start, end, ner_label = span\n",
    "            if ner_label == label:\n",
    "                ner_text = \"\".join(tokens[start:end+1]).replace(\"▁\", \" \").strip().lower()\n",
    "                entity_norm = entity.strip().lower().replace(\"’\", \"\").replace(\"'\", \"\")\n",
    "                if entity_norm == ner_text:\n",
    "                    return (start, end)\n",
    "                # 记录最大区间\n",
    "                if best_span is None or (end - start) > (best_span[1] - best_span[0]):\n",
    "                    best_span = (start, end)\n",
    "        # 如果没有完全匹配，返回最大区间\n",
    "        return best_span\n",
    "\n",
    "    for triple in triples:\n",
    "        sub_span = None\n",
    "        obj_span = None\n",
    "        sub_type = triple.get(\"sub_type\", \"landmark\")\n",
    "        obj_type = \"date\" if looks_like_date(triple[\"obj\"]) else triple.get(\"obj_type\", \"landmark\")\n",
    "        if sub_type in (\"thoroughfare\", \"municipality\") or obj_type in (\"thoroughfare\", \"municipality\"):\n",
    "            continue\n",
    "\n",
    "        if triple[\"rel\"] == \"isLandmarkTypeOf\":\n",
    "            sub_span = get_span_from_ner(triple[\"sub\"], \"caractéristique géographique\")\n",
    "            obj_span = get_span_from_ner(triple[\"obj\"], \"nom géographique\")\n",
    "            if sub_span and obj_span:\n",
    "                relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], triple[\"rel\"]])\n",
    "            continue\n",
    "\n",
    "                \n",
    "        if triple[\"rel\"] in (\"hasGeometryChangeOn\", \"hasNameChangeOn\", \"isNumberedOn\", \"isClassifiedOn\"):\n",
    "            sub_span = get_span_from_ner(triple[\"sub\"], \"nom géographique\")\n",
    "            obj_span = get_span_from_ner(triple[\"obj\"], \"Time\")\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple[\"sub\"], entity_type=sub_type)\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple[\"obj\"], entity_type=obj_type)\n",
    "            if sub_span and sub_span[0] is not None and obj_span and obj_span[0] is not None:\n",
    "                relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], triple[\"rel\"]])\n",
    "            continue\n",
    "\n",
    "        # 其他关系（如 hasOldName、hasNewName、touches、within）保持原逻辑\n",
    "        if triple[\"rel\"] in (\"hasOldName\", \"hasNewName\", \"touches\", \"within\"):\n",
    "            sub_span = get_span_from_ner(triple[\"sub\"], \"nom géographique\")\n",
    "            obj_span = get_span_from_ner(triple[\"obj\"], \"nom géographique\")\n",
    "            if sub_span is None:\n",
    "                sub_span = find_token_span(tokens, sent, triple[\"sub\"], entity_type=sub_type)\n",
    "            if obj_span is None:\n",
    "                obj_span = find_token_span(tokens, sent, triple[\"obj\"], entity_type=obj_type)\n",
    "            if sub_span and sub_span[0] is not None and obj_span and obj_span[0] is not None:\n",
    "                relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], triple[\"rel\"]])\n",
    "            continue\n",
    "\n",
    "        # 其他关系保持原逻辑\n",
    "        sub_span = get_span_from_ner(triple[\"sub\"], \"nom géographique\")\n",
    "        obj_span = get_span_from_ner(triple[\"obj\"], \"Time\")\n",
    "        if sub_span is None:\n",
    "            sub_span = find_token_span(tokens, sent, triple[\"sub\"], entity_type=sub_type)\n",
    "        if obj_span is None:\n",
    "            obj_span = find_token_span(tokens, sent, triple[\"obj\"], entity_type=obj_type)\n",
    "        if sub_span and sub_span[0] is not None and obj_span and obj_span[0] is not None:\n",
    "            relations.append([sub_span[0], sub_span[1], obj_span[0], obj_span[1], triple[\"rel\"]])\n",
    "    return relations\n",
    "\n",
    "\n",
    "def build_ner(sent, tokens, triples=None):\n",
    "    ner = []\n",
    "    geo_prefixes = [\"square\", \"Rue\", \"Place\", \"Boulevard\", \"Quai\", \"Impasse\", \"Passage\", \"Allée\", \"Avenue\", \"Cours\", \"Chemin\"]\n",
    "    if triples:\n",
    "        for triple in triples:\n",
    "            # 只要 obj 不是时间类型就标注\n",
    "            if not looks_like_date(triple[\"obj\"]):\n",
    "                loc_text = triple[\"obj\"]\n",
    "                loc_tokens = tokenizer.tokenize(loc_text)\n",
    "                start, end = find_sublist(tokens, loc_tokens)\n",
    "                if start == -1:\n",
    "                    continue\n",
    "                found = False\n",
    "                for prefix in geo_prefixes:\n",
    "                    prefix_tokens = tokenizer.tokenize(prefix)\n",
    "                    for i in range(len(loc_tokens) - len(prefix_tokens) + 1):\n",
    "                        if [t.lower() for t in loc_tokens[i:i+len(prefix_tokens)]] == [t.lower() for t in prefix_tokens]:\n",
    "                            ner.append([start+i, start+i+len(prefix_tokens)-1, \"caractéristique géographique\"])\n",
    "                            if i+len(prefix_tokens) < len(loc_tokens):\n",
    "                                ner.append([start+i+len(prefix_tokens), end, \"nom\"])\n",
    "                            ner.append([start, end, \"nom géographique\"])\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found:\n",
    "                        break\n",
    "                if not found:\n",
    "                    ner.append([start, end, \"nom géographique\"])\n",
    "                    ner.append([end, end, \"nom\"])\n",
    "    # 事件类型和时间的标注保持不变\n",
    "    event, before_event, event_start, event_end = tag_event_type(sent)\n",
    "    if event and event_start > 0:\n",
    "        event_tokens = tokenizer.tokenize(event)\n",
    "        start, end = find_sublist(tokens, event_tokens)\n",
    "        if start != -1:\n",
    "            ner.append([start, end, \"EventType\"])\n",
    "    time_str, t_start, t_end = extract_time(sent)\n",
    "    if time_str:\n",
    "        sent_before_time = sent[:t_start]\n",
    "        before_tokens = tokenizer.tokenize(sent_before_time, )\n",
    "        time_tokens = tokenizer.tokenize(time_str, )\n",
    "        ner.append([len(before_tokens), len(before_tokens)+len(time_tokens)-1, \"Time\"])\n",
    "    return ner\n",
    "\n",
    "# 修改 process_one_line 调用\n",
    "def process_one_line(d):\n",
    "    sent = d[\"sent\"]\n",
    "    doc_key = d[\"id\"]\n",
    "    tokens = tokenizer.tokenize(sent, )\n",
    "    ner = build_ner(sent, tokens, d.get(\"triples\", []))  # 传递 triples\n",
    "    relations = build_relations(sent, tokens, ner, d.get(\"triples\", []))\n",
    "    return {\n",
    "        \"doc_key\": doc_key,\n",
    "        \"dataset\": \"évolutions d'événements\",\n",
    "        \"sentences\": [tokens],\n",
    "        \"ner\": [ner],\n",
    "        \"relations\": [relations]\n",
    "    }\n",
    "input_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_filtered.jsonl\"\n",
    "output_file = \"C:/Users/jguo/Desktop/PURE-main/data/test_pure.jsonl\"\n",
    "\n",
    "# Traitement batch des données : conversion et annotation\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for data in dataset:\n",
    "        ex = process_one_line(data)\n",
    "        fout.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
